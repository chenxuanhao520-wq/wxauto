"""
AI ç½‘å…³ï¼šæ™ºèƒ½è·¯ç”± + ä¸»å¤‡åˆ‡æ¢ + è‡ªåŠ¨é™çº§
æ”¯æŒå¤šä¸ªå¤§æ¨¡å‹æä¾›å•†ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚åº¦æ™ºèƒ½é€‰æ‹©æœ€ä¼˜æ¨¡å‹
"""
import os
import logging
import asyncio
from typing import Optional, List, Dict, Any

from .types import LLMRequest, LLMResponse, ProviderConfig
from .providers import (
    OpenAIProvider,
    DeepSeekProvider,
    ClaudeProvider,
    QwenProvider,
    GLMProvider,
    ErnieProvider,
    GeminiProvider,
    MoonshotProvider
)
from .base import BaseLLMProvider

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥æ™ºèƒ½è·¯ç”±å™¨
try:
    from .smart_router import SmartModelRouter
    SMART_ROUTER_AVAILABLE = True
except ImportError:
    SMART_ROUTER_AVAILABLE = False
    SmartModelRouter = None


class AIGateway:
    """
    AI ç½‘å…³
    
    åŠŸèƒ½ï¼š
    1. æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®é—®é¢˜å¤æ‚åº¦è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹
    2. ä¸»å¤‡åˆ‡æ¢ï¼šä¸»æ¨¡å‹å¤±è´¥è‡ªåŠ¨é™çº§å¤‡ç”¨æ¨¡å‹
    3. æˆæœ¬ä¼˜åŒ–ï¼šç®€å•é—®é¢˜ç”¨ä¾¿å®œæ¨¡å‹ï¼Œå¤æ‚é—®é¢˜ç”¨å¼ºæ¨¡å‹
    4. å¤šæä¾›å•†ï¼šæ”¯æŒ7ä¸ªå¤§æ¨¡å‹æä¾›å•†
    """
    
    def __init__(
        self,
        primary_provider: str = "qwen",
        primary_model: str = "qwen-turbo",
        fallback_provider: Optional[str] = "deepseek",
        fallback_model: str = "deepseek-chat",
        enable_fallback: bool = True,
        enable_smart_routing: bool = True
    ):
        """
        åˆå§‹åŒ–AIç½‘å…³
        
        Args:
            primary_provider: ä¸»è¦æä¾›å•† (qwen | deepseek | openaiç­‰)
            primary_model: ä¸»è¦æ¨¡å‹
            fallback_provider: å¤‡ç”¨æä¾›å•†
            fallback_model: å¤‡ç”¨æ¨¡å‹
            enable_fallback: æ˜¯å¦å¯ç”¨å¤‡ç”¨é™çº§
            enable_smart_routing: æ˜¯å¦å¯ç”¨æ™ºèƒ½è·¯ç”±
        """
        self.enable_fallback = enable_fallback
        self.enable_smart_routing = enable_smart_routing
        
        # åˆå§‹åŒ–æ™ºèƒ½è·¯ç”±å™¨
        if enable_smart_routing and SMART_ROUTER_AVAILABLE:
            self.router = SmartModelRouter()
            logger.info("âœ… æ™ºèƒ½è·¯ç”±å™¨å·²å¯ç”¨")
        else:
            self.router = None
            if enable_smart_routing and not SMART_ROUTER_AVAILABLE:
                logger.warning("âš ï¸ æ™ºèƒ½è·¯ç”±å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿä¸»å¤‡æ¨¡å¼")
        
        # åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æä¾›å•†å®ä¾‹ï¼ˆç”¨äºæ™ºèƒ½è·¯ç”±ï¼‰
        self.all_providers: Dict[str, BaseLLMProvider] = {}
        if enable_smart_routing:
            self._init_all_providers()
        
        # åˆå§‹åŒ–ä¼ ç»Ÿä¸»å¤‡æä¾›å•†åˆ—è¡¨
        self.providers: List[BaseLLMProvider] = []
        
        # åˆå§‹åŒ–ä¸»æä¾›å•†
        primary = self._create_provider(primary_provider, primary_model)
        if primary and primary.is_available():
            self.providers.append(primary)
            logger.info(f"ä¸»æä¾›å•†å·²åŠ è½½: {primary_provider}/{primary_model}")
        else:
            logger.warning(f"ä¸»æä¾›å•†ä¸å¯ç”¨: {primary_provider}")
        
        # åˆå§‹åŒ–å¤‡ç”¨æä¾›å•†
        if enable_fallback and fallback_provider:
            fallback = self._create_provider(fallback_provider, fallback_model)
            if fallback and fallback.is_available():
                self.providers.append(fallback)
                logger.info(f"å¤‡ç”¨æä¾›å•†å·²åŠ è½½: {fallback_provider}/{fallback_model}")
            else:
                logger.warning(f"å¤‡ç”¨æä¾›å•†ä¸å¯ç”¨: {fallback_provider}")
        
        if not self.providers:
            logger.error("æ²¡æœ‰å¯ç”¨çš„ LLM æä¾›å•†ï¼Œå°†ä½¿ç”¨æ¡©å®ç°")
    
    def _init_all_providers(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æä¾›å•†ï¼ˆç”¨äºæ™ºèƒ½è·¯ç”±ï¼‰"""
        provider_configs = {
            'qwen-turbo': ('qwen', 'qwen-turbo'),
            'qwen-plus': ('qwen', 'qwen-plus'),
            'qwen-max': ('qwen', 'qwen-max'),
            'deepseek': ('deepseek', 'deepseek-chat')
        }
        
        for key, (provider, model) in provider_configs.items():
            try:
                provider_instance = self._create_provider(provider, model)
                if provider_instance and provider_instance.is_available():
                    self.all_providers[key] = provider_instance
                    logger.debug(f"æä¾›å•†å·²æ³¨å†Œ: {key}")
            except Exception as e:
                logger.warning(f"æä¾›å•†æ³¨å†Œå¤±è´¥ {key}: {e}")
    
    async def generate(
        self,
        user_message: str,
        evidence_context: Optional[str] = None,
        session_history: Optional[List] = None,
        max_tokens: int = 512,
        temperature: float = 0.3,
        prompt: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> LLMResponse:
        """
        ç”Ÿæˆå“åº”ï¼ˆæ”¯æŒæ™ºèƒ½è·¯ç”±å’Œè‡ªåŠ¨é™çº§ï¼‰
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            evidence_context: è¯æ®ä¸Šä¸‹æ–‡
            session_history: ä¼šè¯å†å²
            max_tokens: æœ€å¤§ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            prompt: å®Œæ•´æç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼Œä¼˜å…ˆä½¿ç”¨ï¼‰
            metadata: å…ƒæ•°æ®ï¼ˆç”¨äºæ™ºèƒ½è·¯ç”±å†³ç­–ï¼‰
        
        Returns:
            LLMResponse: å“åº”ç»“æœ
        """
        metadata = metadata or {}
        
        # å¦‚æœå¯ç”¨æ™ºèƒ½è·¯ç”±ï¼Œå…ˆé€‰æ‹©æœ€ä¼˜æ¨¡å‹
        selected_provider = None
        routing_info = None
        
        if self.enable_smart_routing and self.router:
            try:
                routing_info = await self.router.route(
                    question=user_message,
                    context=evidence_context,
                    metadata=metadata
                )
                
                model_key = routing_info['model_key']
                
                if model_key in self.all_providers:
                    selected_provider = self.all_providers[model_key]
                    logger.info(
                        f"ğŸ¯ æ™ºèƒ½è·¯ç”±é€‰æ‹©: {model_key} "
                        f"(å¤æ‚åº¦={routing_info.get('complexity', 0):.2f}, "
                        f"åŸå› ={routing_info.get('reason', '')})"
                    )
                else:
                    logger.warning(f"è·¯ç”±é€‰æ‹©çš„æ¨¡å‹ä¸å¯ç”¨: {model_key}ï¼Œä½¿ç”¨é»˜è®¤")
                
            except Exception as e:
                logger.error(f"æ™ºèƒ½è·¯ç”±å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æä¾›å•†")
        
        # æ„å»ºè¯·æ±‚
        request = LLMRequest(
            user_message=user_message,
            evidence_context=evidence_context,
            session_history=session_history,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        # å¦‚æœæ™ºèƒ½è·¯ç”±é€‰æ‹©äº†æ¨¡å‹ï¼Œä¼˜å…ˆå°è¯•
        if selected_provider:
            try:
                # âœ… ä¿®å¤å¼‚æ­¥é˜»å¡ï¼šä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥è°ƒç”¨
                response = await asyncio.to_thread(selected_provider.generate, request)
                
                if response.content and not response.error:
                    # æ·»åŠ è·¯ç”±ä¿¡æ¯åˆ°å“åº”
                    if routing_info:
                        response.routing_info = routing_info
                    return response
                else:
                    logger.warning(
                        f"æ™ºèƒ½è·¯ç”±é€‰æ‹©çš„æ¨¡å‹å¤±è´¥: {selected_provider.name}, "
                        f"error={response.error}ï¼Œå°è¯•é™çº§"
                    )
            
            except Exception as e:
                logger.error(f"æ™ºèƒ½è·¯ç”±æ¨¡å‹è°ƒç”¨å¼‚å¸¸: {selected_provider.name}, {e}ï¼Œå°è¯•é™çº§")
        
        # é™çº§ï¼šä¾æ¬¡å°è¯•ä¸»å¤‡æä¾›å•†
        for i, provider in enumerate(self.providers):
            is_fallback = i > 0
            
            try:
                logger.info(
                    f"è°ƒç”¨ {'å¤‡ç”¨' if is_fallback else 'ä¸»'} æä¾›å•†: {provider.name}"
                )
                
                # âœ… ä¿®å¤å¼‚æ­¥é˜»å¡ï¼šä½¿ç”¨ asyncio.to_thread åŒ…è£…åŒæ­¥è°ƒç”¨
                response = await asyncio.to_thread(provider.generate, request)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if response.content and not response.error:
                    if is_fallback:
                        logger.warning(f"å¤‡ç”¨æä¾›å•†æˆåŠŸ: {provider.name}")
                    return response
                else:
                    logger.warning(
                        f"æä¾›å•†è¿”å›é”™è¯¯: {provider.name}, error={response.error}"
                    )
                    continue
                    
            except Exception as e:
                logger.error(f"æä¾›å•†è°ƒç”¨å¼‚å¸¸: {provider.name}, {e}")
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥ï¼Œè¿”å›æ¡©å“åº”
        logger.error("æ‰€æœ‰ LLM æä¾›å•†éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¡©å“åº”")
        return self._stub_response(user_message)
    
    def _create_provider(
        self,
        provider_name: str,
        model_name: Optional[str] = None
    ) -> Optional[BaseLLMProvider]:
        """
        åˆ›å»ºæä¾›å•†å®ä¾‹
        
        Args:
            provider_name: æä¾›å•†åç§°
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚æœæŒ‡å®šï¼Œè¦†ç›–ç¯å¢ƒå˜é‡ï¼‰
        """
        try:
            # æ¨¡å‹æ˜ å°„ï¼ˆæ”¯æŒæ™ºèƒ½è·¯ç”±çš„å¤šæ¨¡å‹ï¼‰
            model_overrides = {
                'qwen-turbo': 'qwen-turbo',
                'qwen-plus': 'qwen-plus',
                'qwen-max': 'qwen-max',
                'deepseek-chat': 'deepseek-chat'
            }
            
            provider_configs = {
                "openai": {
                    "class": OpenAIProvider,
                    "config": ProviderConfig(
                        name="openai",
                        api_key=os.getenv("OPENAI_API_KEY", ""),
                        api_base=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1"),
                        model=model_name or os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                        timeout=int(os.getenv("OPENAI_TIMEOUT", "30"))
                    )
                },
                "deepseek": {
                    "class": DeepSeekProvider,
                    "config": ProviderConfig(
                        name="deepseek",
                        api_key=os.getenv("DEEPSEEK_API_KEY", ""),
                        api_base=os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com/v1"),
                        model=model_name or os.getenv("DEEPSEEK_MODEL", "deepseek-chat"),
                        timeout=int(os.getenv("DEEPSEEK_TIMEOUT", "30"))
                    )
                },
                "claude": {
                    "class": ClaudeProvider,
                    "config": ProviderConfig(
                        name="claude",
                        api_key=os.getenv("CLAUDE_API_KEY", ""),
                        api_base=os.getenv("CLAUDE_API_BASE", ""),
                        model=model_name or os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022"),
                        timeout=int(os.getenv("CLAUDE_TIMEOUT", "60"))
                    )
                },
                "qwen": {
                    "class": QwenProvider,
                    "config": ProviderConfig(
                        name="qwen",
                        api_key=os.getenv("QWEN_API_KEY", ""),
                        api_base=os.getenv("QWEN_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1"),
                        model=model_name or os.getenv("QWEN_MODEL", "qwen-turbo"),
                        timeout=int(os.getenv("QWEN_TIMEOUT", "30"))
                    )
                },
                "ernie": {
                    "class": ErnieProvider,
                    "config": ProviderConfig(
                        name="ernie",
                        api_key=os.getenv("ERNIE_API_KEY", ""),  # æ ¼å¼: client_id:client_secret
                        api_base=os.getenv("ERNIE_API_BASE", ""),
                        model=model_name or os.getenv("ERNIE_MODEL", "ernie-4.0"),
                        timeout=int(os.getenv("ERNIE_TIMEOUT", "30"))
                    )
                },
                "gemini": {
                    "class": GeminiProvider,
                    "config": ProviderConfig(
                        name="gemini",
                        api_key=os.getenv("GEMINI_API_KEY", ""),
                        api_base=os.getenv("GEMINI_API_BASE", "https://generativelanguage.googleapis.com/v1beta/openai/"),
                        model=model_name or os.getenv("GEMINI_MODEL", "gemini-1.5-flash"),
                        timeout=int(os.getenv("GEMINI_TIMEOUT", "30"))
                    )
                },
                "moonshot": {
                    "class": MoonshotProvider,
                    "config": ProviderConfig(
                        name="moonshot",
                        api_key=os.getenv("MOONSHOT_API_KEY", ""),
                        api_base=os.getenv("MOONSHOT_API_BASE", "https://api.moonshot.cn/v1"),
                        model=model_name or os.getenv("MOONSHOT_MODEL", "moonshot-v1-8k"),
                        timeout=int(os.getenv("MOONSHOT_TIMEOUT", "30"))
                    )
                },
                "glm": {
                    "class": GLMProvider,
                    "config": ProviderConfig(
                        name="glm",
                        api_key=os.getenv("GLM_API_KEY", ""),
                        api_base=os.getenv("GLM_API_BASE", "https://open.bigmodel.cn/api/paas/v4"),
                        model=model_name or os.getenv("GLM_MODEL", "glm-4-flash"),
                        timeout=int(os.getenv("GLM_TIMEOUT", "30"))
                    )
                }
            }
            
            if provider_name not in provider_configs:
                logger.error(f"æœªçŸ¥çš„æä¾›å•†: {provider_name}")
                return None
            
            provider_info = provider_configs[provider_name]
            return provider_info["class"](provider_info["config"])
                
        except Exception as e:
            logger.error(f"åˆ›å»ºæä¾›å•†å¤±è´¥: {provider_name}, {e}")
            return None
    
    def _stub_response(self, user_message: str) -> LLMResponse:
        """æ¡©å“åº”ï¼ˆå…œåº•ï¼‰"""
        return LLMResponse(
            content="æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œå·²ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœã€‚",
            provider="stub",
            model="stub",
            token_in=len(user_message) * 2,
            token_out=50,
            token_total=len(user_message) * 2 + 50,
            latency_ms=0,
            error="All providers failed"
        )
    
    def health_check(self) -> dict:
        """å¥åº·æ£€æŸ¥"""
        status = {
            "providers": [],
            "available": len(self.providers) > 0,
            "smart_routing_enabled": self.enable_smart_routing and self.router is not None,
            "total_providers": len(self.all_providers) if self.all_providers else len(self.providers)
        }
        
        # ä¸»å¤‡æä¾›å•†çŠ¶æ€
        for provider in self.providers:
            status["providers"].append({
                "name": provider.name,
                "available": provider.is_available(),
                "type": "primary" if provider == self.providers[0] else "fallback"
            })
        
        # æ™ºèƒ½è·¯ç”±æä¾›å•†çŠ¶æ€
        if self.all_providers:
            status["routing_providers"] = [
                {
                    "key": key,
                    "name": provider.name,
                    "available": provider.is_available()
                }
                for key, provider in self.all_providers.items()
            ]
        
        return status
    
    def get_routing_stats(self) -> Dict[str, Any]:
        """è·å–è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
        if not self.router:
            return {"smart_routing_enabled": False}
        
        return {
            "smart_routing_enabled": True,
            "router_stats": self.router.get_model_stats()
        }

