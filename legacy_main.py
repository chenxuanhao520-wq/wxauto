"""
ä¸»ç¨‹åºï¼šç›‘å¬ â†’ é™å™ª â†’ RAG â†’ LLM â†’ å‘é€ â†’ è½åº“
Phase 0-1: å®ç°ç›‘å¬ã€@è¯†åˆ«ã€å»é‡ã€é¢‘æ§ã€ACKã€åˆ†æµä¸è½åº“
Phase 3: æ¥å…¥çœŸå® AI ç½‘å…³
"""

# å¼ºåˆ¶ UTF-8 ç¼–ç ï¼ˆè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰
import sys
import logging

# é‡æ–°é…ç½®æ ‡å‡†è¾“å‡ºä¸º UTF-8
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# é…ç½®æ—¥å¿—ä¸º UTF-8
logging.basicConfig(encoding='utf-8', level=logging.INFO)

import os
import time
import uuid
import yaml
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œæ¨¡å—è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'core'))
sys.path.insert(0, str(project_root / 'modules'))

from modules.adapters.wxauto_adapter import Message, FakeWxAdapter
from modules.storage.db import Database, MessageLog, SessionInfo
from modules.rag.retriever import Retriever, Evidence
from modules.ai_gateway.gateway import AIGateway
from core.conversation_tracker import ConversationTracker, ConversationOutcome
from modules.adaptive_learning import UserProfiler, PersonalizedPromptGenerator, ContinuousLearner
from core.customer_service_adapter import customer_manager, init_default_groups
from core.smart_analyzer import smart_analyzer

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
Path("logs").mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


class CustomerServiceBot:
    """å®¢æœä¸­å°ä¸»ç¨‹åº"""
    
    def __init__(self, config_path: str = "config.yaml", use_fake: bool = True):
        """
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            use_fake: æ˜¯å¦ä½¿ç”¨å‡é€‚é…å™¨ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
        """
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        self.use_fake = use_fake
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.db = Database(self.config['database']['path'])
        self.db.init_database()
        
        # åˆå§‹åŒ–å®¢æˆ·ç®¡ç†ç³»ç»Ÿ
        init_default_groups()
        logger.info("å®¢æˆ·ç®¡ç†ç³»ç»Ÿå·²åˆå§‹åŒ–")
        
        # å¾®ä¿¡é€‚é…å™¨
        whitelisted_groups = self.config['wechat']['whitelisted_groups']
        if use_fake:
            self.wx_adapter = FakeWxAdapter(whitelisted_groups)
            logger.info("ä½¿ç”¨ FakeWxAdapterï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
        else:
            # Phase 1 çœŸå®ç¯å¢ƒæ›¿æ¢
            from adapters.wxauto_adapter import WxAutoAdapter
            self.wx_adapter = WxAutoAdapter(whitelisted_groups)
            logger.info("ä½¿ç”¨ WxAutoAdapterï¼ˆçœŸå®æ¨¡å¼ï¼‰")
        
        # RAG æ£€ç´¢å™¨
        rag_config = self.config['rag']
        self.retriever = Retriever(
            bm25_topn=rag_config['bm25_topn'],
            top_k=rag_config['top_k'],
            min_confidence=rag_config['min_confidence']
        )
        
        # å°è¯•åŠ è½½çŸ¥è¯†åº“
        try:
            self.retriever.load_knowledge_base(self.config['database']['path'])
        except Exception as e:
            logger.warning(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {e}")
        
        # AI ç½‘å…³ï¼ˆPhase 3ï¼‰
        try:
            llm_config = self.config['llm']
            self.ai_gateway = AIGateway(
                primary_provider=llm_config['primary'].split(':')[0],  # 'openai:gpt-4o-mini' -> 'openai'
                fallback_provider=llm_config.get('fallback', '').split(':')[0] if llm_config.get('fallback') else None,
                enable_fallback=True
            )
            logger.info("AI ç½‘å…³å·²åˆå§‹åŒ–")
        except Exception as e:
            logger.warning(f"AI ç½‘å…³åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¡©å®ç°: {e}")
            self.ai_gateway = None
        
        # å¯¹è¯è¿½è¸ªå™¨ï¼ˆPhase 3.1ï¼‰
        try:
            self.conversation_tracker = ConversationTracker(self.db)
            logger.info("å¯¹è¯è¿½è¸ªå™¨å·²åˆå§‹åŒ–")
        except Exception as e:
            logger.warning(f"å¯¹è¯è¿½è¸ªå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.conversation_tracker = None
        
        # è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿï¼ˆPhase 3.2ï¼‰
        try:
            self.user_profiler = UserProfiler(self.db)
            self.prompt_generator = PersonalizedPromptGenerator()
            self.continuous_learner = ContinuousLearner(self.db, self.user_profiler)
            
            # åŠ è½½åŸºç¡€é£æ ¼ï¼ˆå¦‚æœæœ‰ï¼‰
            import json
            style_file = Path("data/conversation_style.json")
            if style_file.exists():
                with open(style_file, 'r', encoding='utf-8') as f:
                    base_style = json.load(f)
                    self.prompt_generator = PersonalizedPromptGenerator(base_style)
                logger.info("å¯¹è¯é£æ ¼å·²åŠ è½½")
            
            logger.info("è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿå·²åˆå§‹åŒ–")
            self.adaptive_learning_enabled = True
        except Exception as e:
            logger.warning(f"è‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.adaptive_learning_enabled = False
        
        # é˜ˆå€¼é…ç½®
        self.confidence_thresholds = self.config['confidence']
        self.ack_config = self.config['ack']
        self.rate_limit_config = self.config['rate_limit']
        self.session_config = self.config['session']
        
        # ç¦ç­”åŸŸ
        self.forbidden_topics = self.config['forbidden_topics']
        
        # ç®¡ç†å‘˜é…ç½®
        self.admin_config = self.config['admin']
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.shadow_mode = False  # å½±å­æ¨¡å¼ï¼šåªè®°å½•ä¸å‘è¨€
        self.global_mute = False  # å…¨å±€é™é»˜
        self.debug_mode = False  # è°ƒè¯•æ¨¡å¼
        
        logger.info("CustomerServiceBot åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # åº”ç”¨ç¯å¢ƒç‰¹å®šé…ç½®
        env = config.get('env', 'dev')
        if env in config.get('profiles', {}):
            profile = config['profiles'][env]
            # æ·±åº¦åˆå¹¶é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
            for key, value in profile.items():
                if isinstance(value, dict) and key in config:
                    config[key].update(value)
                else:
                    config[key] = value
        
        logger.info(f"é…ç½®å·²åŠ è½½: env={env}")
        return config
    
    def run(self):
        """ä¸»å¾ªç¯ï¼šç›‘å¬æ¶ˆæ¯å¹¶å¤„ç†"""
        self.is_running = True
        check_interval = self.config['wechat']['check_interval_ms'] / 1000
        
        logger.info("=" * 60)
        logger.info("å®¢æœä¸­å°å·²å¯åŠ¨ï¼Œå¼€å§‹ç›‘å¬æ¶ˆæ¯...")
        logger.info(f"ç™½åå•ç¾¤èŠ: {self.config['wechat']['whitelisted_groups']}")
        logger.info(f"æ£€æŸ¥é—´éš”: {check_interval}s")
        logger.info("=" * 60)
        
        try:
            while self.is_running:
                try:
                    # æ¸…ç†è¿‡æœŸä¼šè¯
                    self.db.expire_old_sessions()
                    
                    # è¿­ä»£æ–°æ¶ˆæ¯
                    for msg in self.wx_adapter.iter_new_messages():
                        self._process_message(msg)
                    
                    # é—´éš”ç­‰å¾…
                    time.sleep(check_interval)
                    
                except KeyboardInterrupt:
                    logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
                    break
                except Exception as e:
                    logger.error(f"ä¸»å¾ªç¯å¼‚å¸¸: {e}", exc_info=True)
                    time.sleep(check_interval * 2)  # å‡ºé”™ååŠ å€ç­‰å¾…
                    
        finally:
            self.stop()
    
    def _process_message(self, msg: Message) -> None:
        """
        å¤„ç†å•æ¡æ¶ˆæ¯çš„å®Œæ•´æµç¨‹
        Args:
            msg: å¾®ä¿¡æ¶ˆæ¯
        """
        start_time = time.time()
        request_id = str(uuid.uuid4())
        
        logger.info(
            f"[{request_id[:8]}] å¼€å§‹å¤„ç†æ¶ˆæ¯: "
            f"group={msg.group_name}, sender={msg.sender_name}, "
            f"content={msg.content[:50]}..."
        )
        
        try:
            # Step 1: ç®¡ç†æŒ‡ä»¤æ£€æŸ¥
            if self._handle_admin_command(msg):
                logger.info(f"[{request_id[:8]}] ç®¡ç†æŒ‡ä»¤å·²å¤„ç†")
                return
            
            # Step 2: å»é‡æ£€æŸ¥
            if self.db.check_duplicate(
                msg.group_id, msg.sender_id, msg.content,
                window_seconds=10
            ):
                logger.info(f"[{request_id[:8]}] æ¶ˆæ¯é‡å¤ï¼Œå·²å¿½ç•¥")
                return
            
            # Step 3: é€Ÿç‡é™åˆ¶
            rate_limit_result = self._check_rate_limits(msg)
            if not rate_limit_result['allowed']:
                self._handle_rate_limited(msg, request_id, rate_limit_result)
                return
            
            # Step 4: ç¦ç­”åŸŸæ£€æŸ¥
            if self._is_forbidden_topic(msg.content):
                self._handle_forbidden(msg, request_id)
                return
            
            # Step 5: å‘é€ ACK
            if self.ack_config['enabled'] and not self.shadow_mode:
                ack_success = self.wx_adapter.ack(
                    msg.group_name,
                    msg.sender_name,
                    self.ack_config['message']
                )
                logger.info(f"[{request_id[:8]}] ACK å‘é€: {'æˆåŠŸ' if ack_success else 'å¤±è´¥'}")
            
            # Step 6: å®¢æˆ·ç®¡ç†
            customer = self._get_or_create_customer(msg)
            if not customer:
                logger.warning(f"[{request_id[:8]}] å®¢æˆ·è¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡å¤„ç†")
                return
            
            logger.info(f"[{request_id[:8]}] å®¢æˆ·ä¿¡æ¯: {customer.customer_id} ({customer.name})")
            
            # Step 7: ä¼šè¯ç®¡ç†
            session_key = f"{msg.group_id}:{msg.sender_id}"
            session = self.db.upsert_session(
                session_key=session_key,
                group_id=msg.group_id,
                sender_id=msg.sender_id,
                sender_name=msg.sender_name,
                ttl_minutes=self.session_config['ttl_minutes']
            )
            
            # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œè‡ªåŠ¨åˆ†ç±»æ ‡ç­¾
            if session.turn_count == 1 and self.conversation_tracker:
                tags = self._auto_classify_tags(msg.content)
                self.conversation_tracker.start_conversation(session_key, tags)
            
            # è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ°å¯¹è¯ä¸²
            if self.conversation_tracker:
                self.conversation_tracker.add_message_to_thread(
                    session_key=session_key,
                    message_id=request_id,
                    role="user",
                    content=msg.content
                )
            
            logger.debug(
                f"[{request_id[:8]}] ä¼šè¯: key={session_key}, "
                f"turn={session.turn_count}"
            )
            
            # Step 8: RAG æ£€ç´¢
            retrieval_start = time.time()
            evidences = self.retriever.retrieve(msg.content)
            confidence = self.retriever.calculate_confidence(evidences)
            retrieval_time = int((time.time() - retrieval_start) * 1000)
            
            logger.info(
                f"[{request_id[:8]}] RAG æ£€ç´¢å®Œæˆ: "
                f"evidences={len(evidences)}, confidence={confidence:.2f}, "
                f"time={retrieval_time}ms"
            )
            
            # Step 9: æ™ºèƒ½åˆ†æ
            analysis_start = time.time()
            knowledge_result = {
                'documents': [ev.content for ev in evidences],
                'confidence': confidence,
                'evidence_summary': '; '.join([ev.content[:100] + '...' for ev in evidences[:3]])
            }
            
            analysis = smart_analyzer.deep_think_analysis(customer, msg.content, knowledge_result)
            analysis_time = int((time.time() - analysis_start) * 1000)
            
            logger.info(
                f"[{request_id[:8]}] æ™ºèƒ½åˆ†æå®Œæˆ: "
                f"type={analysis.question_type}, urgency={analysis.urgency_level}, "
                f"complexity={analysis.complexity}, time={analysis_time}ms"
            )
            
            # Step 10: ç½®ä¿¡åº¦åˆ†æµï¼ˆç»“åˆåˆ†æç»“æœï¼‰
            branch = self._determine_branch(confidence, analysis)
            
            logger.info(f"[{request_id[:8]}] åˆ†æµå†³ç­–: branch={branch}, conf={confidence:.2f}")
            
            # Step 11: ç”Ÿæˆæ™ºèƒ½å“åº”
            generation_start = time.time()
            response_text, llm_info = self._generate_smart_response(
                msg, customer, analysis, evidences, confidence, branch, session
            )
            generation_time = int((time.time() - generation_start) * 1000)
            
            # Step 10: å‘é€å“åº”
            send_start = time.time()
            if not self.shadow_mode and response_text:
                send_success = self.wx_adapter.send_text(
                    msg.group_name,
                    response_text,
                    at_user=msg.sender_name
                )
            else:
                send_success = True  # å½±å­æ¨¡å¼è§†ä¸ºæˆåŠŸ
            send_time = int((time.time() - send_start) * 1000)
            
            # è®°å½•AIå›å¤åˆ°å¯¹è¯ä¸²
            if self.conversation_tracker and response_text:
                self.conversation_tracker.add_message_to_thread(
                    session_key=session_key,
                    message_id=request_id,
                    role="assistant",
                    content=response_text,
                    metadata={
                        'confidence': confidence,
                        'provider': llm_info.get('provider'),
                        'model': llm_info.get('model'),
                        'tokens': llm_info.get('token_total')
                    }
                )
            
            # Step 11: è½åº“
            total_time = int((time.time() - start_time) * 1000)
            
            message_log = MessageLog(
                request_id=request_id,
                session_id=session.id,
                group_id=msg.group_id,
                group_name=msg.group_name,
                sender_id=msg.sender_id,
                sender_name=msg.sender_name,
                user_message=msg.content,
                bot_response=response_text,
                evidence_ids=str([e.chunk_id for e in evidences]),
                evidence_summary=self.retriever.format_evidence_summary(evidences),
                confidence=confidence,
                branch=branch,
                provider=llm_info.get('provider', 'stub'),
                model=llm_info.get('model', 'stub'),
                token_in=llm_info.get('token_in', 0),
                token_out=llm_info.get('token_out', 0),
                token_total=llm_info.get('token_total', 0),
                latency_receive_ms=0,
                latency_retrieval_ms=retrieval_time,
                latency_generation_ms=generation_time,
                latency_send_ms=send_time,
                latency_total_ms=total_time,
                status='answered' if send_success else 'failed',
                received_at=msg.timestamp,
                responded_at=datetime.now()
            )
            
            self.db.log_message(message_log)
            
            # è‡ªåŠ¨è¯„ä¼°å¯¹è¯æ•ˆæœï¼ˆå¦‚æœæ˜¯ä¼šè¯ç»“æŸæˆ–è½¬äººå·¥ï¼‰
            if self.conversation_tracker:
                should_evaluate = (
                    branch == 'handoff' or  # è½¬äººå·¥
                    not send_success or      # å‘é€å¤±è´¥
                    session.turn_count >= 10  # å¯¹è¯è¿‡é•¿
                )
                
                if should_evaluate:
                    outcome = self.conversation_tracker.auto_evaluate_outcome(
                        session_key=session_key,
                        last_branch=branch,
                        last_status='answered' if send_success else 'failed',
                        avg_confidence=confidence
                    )
                    self.conversation_tracker.mark_outcome(session_key, outcome)
                    
                    logger.info(
                        f"[{request_id[:8]}] å¯¹è¯è¯„ä¼°: "
                        f"outcome={outcome.outcome}, resolved_by={outcome.resolved_by}"
                    )
                    
                    # æŒç»­å­¦ä¹ ï¼šä»å¯¹è¯ä¸­å­¦ä¹ 
                    if self.adaptive_learning_enabled and outcome.satisfaction_score:
                        try:
                            conversation_data = {
                                'user_message': msg.content,
                                'bot_response': response_text,
                                'satisfaction_score': outcome.satisfaction_score,
                                'confidence': confidence
                            }
                            self.continuous_learner.learn_from_conversation(
                                user_id=msg.sender_id,
                                conversation=conversation_data,
                                satisfaction_score=outcome.satisfaction_score
                            )
                        except Exception as e:
                            logger.warning(f"æŒç»­å­¦ä¹ å¤±è´¥: {e}")
            
            logger.info(
                f"[{request_id[:8]}] å¤„ç†å®Œæˆ: "
                f"branch={branch}, total_time={total_time}ms, "
                f"status={'answered' if send_success else 'failed'}"
            )
            
        except Exception as e:
            logger.error(f"[{request_id[:8]}] å¤„ç†å¤±è´¥: {e}", exc_info=True)
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            try:
                error_log = MessageLog(
                    request_id=request_id,
                    group_id=msg.group_id,
                    sender_id=msg.sender_id,
                    user_message=msg.content,
                    status='failed',
                    error_message=str(e),
                    received_at=msg.timestamp
                )
                self.db.log_message(error_log)
            except Exception as db_error:
                logger.error(f"è®°å½•é”™è¯¯æ—¥å¿—å¤±è´¥: {db_error}")
    
    def _check_rate_limits(self, msg: Message) -> Dict[str, Any]:
        """
        æ£€æŸ¥é€Ÿç‡é™åˆ¶
        Returns:
            {'allowed': bool, 'reason': str, 'count': int}
        """
        # æ£€æŸ¥ç¾¤çº§åˆ«é™åˆ¶
        group_allowed, group_count = self.db.check_rate_limit(
            'group',
            msg.group_id,
            self.rate_limit_config['per_group_per_minute'],
            60
        )
        
        if not group_allowed:
            return {
                'allowed': False,
                'reason': 'group_limit_exceeded',
                'count': group_count
            }
        
        # æ£€æŸ¥ç”¨æˆ·çº§åˆ«é™åˆ¶
        user_key = f"{msg.group_id}:{msg.sender_id}"
        user_allowed, user_count = self.db.check_rate_limit(
            'user',
            user_key,
            self.rate_limit_config['per_user_per_30s'],
            30
        )
        
        if not user_allowed:
            return {
                'allowed': False,
                'reason': 'user_limit_exceeded',
                'count': user_count
            }
        
        return {'allowed': True, 'reason': '', 'count': 0}
    
    def _is_forbidden_topic(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç¦ç­”åŸŸ"""
        for keyword in self.forbidden_topics:
            if keyword in content:
                logger.warning(f"æ£€æµ‹åˆ°ç¦ç­”åŸŸå…³é”®è¯: {keyword}")
                return True
        return False
    
    def _determine_branch(self, confidence: float) -> str:
        """
        æ ¹æ®ç½®ä¿¡åº¦å†³å®šåˆ†æ”¯
        Returns:
            'direct_answer' | 'clarification' | 'handoff'
        """
        if confidence >= self.confidence_thresholds['direct_answer']:
            return 'direct_answer'
        elif confidence >= self.confidence_thresholds['clarification']:
            return 'clarification'
        else:
            return 'handoff'
    
    def _generate_response(
        self,
        msg: Message,
        evidences: list,
        confidence: float,
        branch: str,
        session: SessionInfo
    ) -> tuple[Optional[str], Dict[str, Any]]:
        """
        ç”Ÿæˆå“åº”ï¼ˆä½¿ç”¨ AI ç½‘å…³æˆ–æ¡©å®ç°ï¼‰
        Returns:
            (response_text, llm_info)
        """
        # å¦‚æœåˆ†æ”¯æ˜¯è½¬äººå·¥æˆ–æ¾„æ¸…ï¼Œä½¿ç”¨æ¨¡æ¿
        if branch == 'handoff':
            response = (
                "æ‚¨çš„é—®é¢˜éœ€è¦ä¸“ä¸šæ”¯æŒï¼Œå·²ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœã€‚\n"
                "å€¼ç­åŒäº‹ç¨åä¼šä¸æ‚¨è”ç³»ï¼Œè¯·ç¨å€™ã€‚"
            )
            llm_info = {
                'provider': 'template',
                'model': 'handoff',
                'token_in': 0,
                'token_out': len(response),
                'token_total': len(response)
            }
            return response, llm_info
        
        elif branch == 'clarification':
            response = (
                "ä¸ºäº†æ›´å‡†ç¡®åœ°å¸®åŠ©æ‚¨ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š\n"
                "â‘  è®¾å¤‡å‹å·\n"
                "â‘¡ å›ºä»¶ç‰ˆæœ¬\n"
                "â‘¢ ç°åœºç”µæºå‚æ•°ï¼ˆå¦‚é€‚ç”¨ï¼‰"
            )
            llm_info = {
                'provider': 'template',
                'model': 'clarification',
                'token_in': 0,
                'token_out': len(response),
                'token_total': len(response)
            }
            return response, llm_info
        
        # ç›´ç­”åˆ†æ”¯ï¼šä½¿ç”¨ AI ç½‘å…³ï¼ˆä¸ªæ€§åŒ–ï¼‰
        if self.ai_gateway and evidences:
            try:
                # æ„å»ºè¯æ®ä¸Šä¸‹æ–‡
                evidence_context = self.retriever.format_evidence_summary(evidences)
                evidence_text = "\n\n".join([
                    f"ã€{ev.document_name} {ev.document_version} - {ev.section}ã€‘\n{ev.content}"
                    for ev in evidences[:2]  # åªç”¨å‰2æ¡è¯æ®
                ])
                
                # è·å–ç”¨æˆ·ç”»åƒå¹¶ç”Ÿæˆä¸ªæ€§åŒ–Prompt
                system_prompt = None
                if self.adaptive_learning_enabled:
                    try:
                        # è·å–æˆ–åˆ›å»ºç”¨æˆ·ç”»åƒ
                        user_profile = self.user_profiler.get_or_create_profile(
                            msg.sender_id,
                            msg.sender_name
                        )
                        
                        # ç”Ÿæˆä¸ªæ€§åŒ–Prompt
                        system_prompt = self.prompt_generator.generate(
                            user_profile=user_profile,
                            context="å……ç”µæ¡©å®¢æœ"
                        )
                        
                        logger.debug(
                            f"ä½¿ç”¨ä¸ªæ€§åŒ–Prompt: user={msg.sender_id}, "
                            f"type={user_profile.customer_type}, "
                            f"style={user_profile.communication_style}"
                        )
                    except Exception as e:
                        logger.warning(f"ä¸ªæ€§åŒ–Promptç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤: {e}")
                
                # è°ƒç”¨ AI ç½‘å…³ï¼ˆå¸¦ä¸ªæ€§åŒ–Promptï¼‰
                llm_response = self.ai_gateway.generate(
                    user_message=msg.content,
                    evidence_context=evidence_text,
                    session_history=None,  # TODO: åç»­æ·»åŠ ä¼šè¯å†å²
                    max_tokens=self.config['llm']['max_tokens'],
                    temperature=self.config['llm']['temperature']
                )
                
                # å¦‚æœæœ‰ä¸ªæ€§åŒ–Promptï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„
                if system_prompt and hasattr(llm_response, 'request'):
                    # åœ¨å®é™…è°ƒç”¨ä¸­è®¾ç½®system_prompt
                    pass  # è¿™é‡Œéœ€è¦AIç½‘å…³æ”¯æŒè‡ªå®šä¹‰system_prompt
                
                if llm_response.content and not llm_response.error:
                    # AI ç”ŸæˆæˆåŠŸ
                    llm_info = {
                        'provider': llm_response.provider,
                        'model': llm_response.model,
                        'token_in': llm_response.token_in,
                        'token_out': llm_response.token_out,
                        'token_total': llm_response.token_total
                    }
                    return llm_response.content, llm_info
                else:
                    # AI ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°æ¨¡æ¿
                    logger.warning(f"AI ç”Ÿæˆå¤±è´¥: {llm_response.error}")
            
            except Exception as e:
                logger.error(f"AI ç½‘å…³è°ƒç”¨å¼‚å¸¸: {e}")
        
        # å›é€€åˆ°æ¡©å®ç°
        if evidences:
            ev = evidences[0]
            response = (
                f"æ ¹æ®ã€Š{ev.document_name} v{ev.document_version}ã€‹ï¼š\n"
                f"â‘  {ev.content}\n"
                f"â‘¡ é‡ç‚¹å…³æ³¨ {ev.section} ä¸­çš„å®‰å…¨æç¤º\n"
                f"â‘¢ å®Œæˆæ“ä½œåè¯·åé¦ˆæ•ˆæœ\n"
                f"è‹¥åœºæ™¯ä¸åŒè¯·è¡¥å……å‹å·/ç‰ˆæœ¬/å‚æ•°"
            )
        else:
            response = "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œæ­£åœ¨ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœ..."
        
        llm_info = {
            'provider': 'stub',
            'model': 'stub',
            'token_in': len(msg.content) * 2,
            'token_out': len(response) * 2,
            'token_total': len(msg.content) * 2 + len(response) * 2
        }
        
        return response, llm_info
    
    def _handle_rate_limited(
        self,
        msg: Message,
        request_id: str,
        limit_result: Dict[str, Any]
    ) -> None:
        """å¤„ç†é€Ÿç‡é™åˆ¶"""
        reason = limit_result['reason']
        count = limit_result['count']
        
        logger.warning(
            f"[{request_id[:8]}] é€Ÿç‡é™åˆ¶è§¦å‘: {reason}, count={count}"
        )
        
        # æ¸©å’Œæç¤º
        if not self.shadow_mode:
            self.wx_adapter.send_text(
                msg.group_name,
                "æ‚¨çš„æé—®é¢‘ç‡ç¨å¿«ï¼Œè¯·ç¨åå†è¯•ï½",
                at_user=msg.sender_name
            )
        
        # è®°å½•æ—¥å¿—
        log = MessageLog(
            request_id=request_id,
            group_id=msg.group_id,
            sender_id=msg.sender_id,
            user_message=msg.content,
            branch='rate_limited',
            status='ignored',
            received_at=msg.timestamp
        )
        self.db.log_message(log)
    
    def _handle_forbidden(self, msg: Message, request_id: str) -> None:
        """å¤„ç†ç¦ç­”åŸŸ"""
        logger.warning(f"[{request_id[:8]}] ç¦ç­”åŸŸè§¦å‘")
        
        # è½¬äººå·¥
        if not self.shadow_mode:
            self.wx_adapter.send_text(
                msg.group_name,
                "æ­¤ç±»é—®é¢˜éœ€è¦ä¸“ä¸šé¡¾é—®ååŠ©ï¼Œå·²ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœã€‚",
                at_user=msg.sender_name
            )
        
        # è®°å½•æ—¥å¿—
        log = MessageLog(
            request_id=request_id,
            group_id=msg.group_id,
            sender_id=msg.sender_id,
            user_message=msg.content,
            branch='handoff',
            handoff_reason='policy',
            status='answered',
            bot_response='å·²è½¬äººå·¥ï¼ˆç¦ç­”åŸŸï¼‰',
            received_at=msg.timestamp
        )
        self.db.log_message(log)
    
    def _handle_admin_command(self, msg: Message) -> bool:
        """
        å¤„ç†ç®¡ç†æŒ‡ä»¤
        Returns:
            bool: æ˜¯å¦ä¸ºç®¡ç†æŒ‡ä»¤
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†å‘˜
        if msg.sender_name not in self.admin_config['names']:
            return False
        
        content = msg.content.strip()
        commands = self.admin_config['commands']
        
        # #mute
        if content == commands['mute']:
            self.global_mute = True
            self.db.set_config('global_mute', 'true')
            self.wx_adapter.send_text(msg.group_name, "âœ… å·²å¼€å¯å…¨å±€é™é»˜")
            return True
        
        # #unmute
        if content == commands['unmute']:
            self.global_mute = False
            self.db.set_config('global_mute', 'false')
            self.wx_adapter.send_text(msg.group_name, "âœ… å·²å…³é—­å…¨å±€é™é»˜")
            return True
        
        # #status
        if content == commands['status']:
            status_text = self._get_status_report()
            self.wx_adapter.send_text(msg.group_name, status_text)
            return True
        
        # #debug on/off
        if content == commands['debug_on']:
            self.debug_mode = True
            logging.getLogger().setLevel(logging.DEBUG)
            self.wx_adapter.send_text(msg.group_name, "âœ… è°ƒè¯•æ¨¡å¼å·²å¼€å¯")
            return True
        
        if content == commands['debug_off']:
            self.debug_mode = False
            logging.getLogger().setLevel(logging.INFO)
            self.wx_adapter.send_text(msg.group_name, "âœ… è°ƒè¯•æ¨¡å¼å·²å…³é—­")
            return True
        
        return False
    
    def _get_status_report(self) -> str:
        """ç”ŸæˆçŠ¶æ€æŠ¥å‘Š"""
        # ç®€åŒ–çŠ¶æ€æŠ¥å‘Šï¼ˆPhase 4 å¯æ‰©å±•ï¼‰
        shadow_status = "æ˜¯" if self.shadow_mode else "å¦"
        mute_status = "æ˜¯" if self.global_mute else "å¦"
        debug_status = "æ˜¯" if self.debug_mode else "å¦"
        
        return (
            f"ğŸ“Š ç³»ç»ŸçŠ¶æ€\n"
            f"å½±å­æ¨¡å¼: {shadow_status}\n"
            f"å…¨å±€é™é»˜: {mute_status}\n"
            f"è°ƒè¯•æ¨¡å¼: {debug_status}\n"
            f"è¿è¡Œä¸­: æ˜¯"
        )
    
    def _auto_classify_tags(self, content: str) -> List[str]:
        """
        æ ¹æ®æ¶ˆæ¯å†…å®¹è‡ªåŠ¨åˆ†ç±»æ ‡ç­¾
        
        Args:
            content: æ¶ˆæ¯å†…å®¹
        
        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        tags = []
        
        # å…³é”®è¯åŒ¹é…
        keyword_tags = {
            'å”®å': ['å”®å', 'ä¿ä¿®', 'ç»´ä¿®', 'é€€æ¢', 'è´¨é‡é—®é¢˜'],
            'æŠ€æœ¯æ”¯æŒ': ['å®‰è£…', 'é…ç½®', 'ä½¿ç”¨', 'æ•…éšœ', 'é—®é¢˜', 'æŠ¥é”™', 'å¼‚å¸¸'],
            'ä»·æ ¼å’¨è¯¢': ['ä»·æ ¼', 'å¤šå°‘é’±', 'è´¹ç”¨', 'æŠ¥ä»·', 'ä¼˜æƒ '],
            'äº§å“å’¨è¯¢': ['äº§å“', 'å‹å·', 'å‚æ•°', 'è§„æ ¼', 'åŠŸèƒ½'],
            'å®‰è£…é—®é¢˜': ['å®‰è£…', 'ç»„è£…', 'éƒ¨ç½²'],
            'æ•…éšœæ’æŸ¥': ['æ•…éšœ', 'ä¸å·¥ä½œ', 'æ— æ³•', 'æŠ¥é”™', 'å¼‚å¸¸'],
        }
        
        content_lower = content.lower()
        
        for tag, keywords in keyword_tags.items():
            if any(kw in content_lower for kw in keywords):
                tags.append(tag)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ ‡ç­¾ï¼Œæ·»åŠ "ä¸€èˆ¬å’¨è¯¢"
        if not tags:
            tags.append('ä¸€èˆ¬å’¨è¯¢')
        
        return tags
    
    def _get_or_create_customer(self, msg: Message):
        """è·å–æˆ–åˆ›å»ºå®¢æˆ·"""
        # é¦–å…ˆå°è¯•æ ¹æ®å§“åå’Œç¾¤èŠæŸ¥æ‰¾ç°æœ‰å®¢æˆ·
        customer = customer_manager.find_customer_by_name(msg.sender_name, msg.group_name)
        
        if customer:
            # æ›´æ–°å®¢æˆ·æ´»åŠ¨
            customer_manager.update_customer_activity(customer.customer_id)
            return customer
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ›å»ºæ–°å®¢æˆ·
        try:
            # è·å–ç¾¤èŠåˆ†ç±»
            group_classification = customer_manager.get_group_classification(msg.group_name)
            priority = group_classification.priority if group_classification else 3
            
            # æ³¨å†Œæ–°å®¢æˆ·
            customer_id = customer_manager.register_customer(
                name=msg.sender_name,
                group_name=msg.group_name,
                notes=f"è‡ªåŠ¨æ³¨å†Œ - é¦–æ¬¡å’¨è¯¢",
                priority=priority
            )
            
            logger.info(f"æ–°å®¢æˆ·æ³¨å†ŒæˆåŠŸ: {customer_id} ({msg.sender_name})")
            return customer_manager.get_customer(customer_id)
            
        except Exception as e:
            logger.error(f"å®¢æˆ·æ³¨å†Œå¤±è´¥: {e}")
            return None
    
    def _determine_branch(self, confidence: float, analysis=None) -> str:
        """ç¡®å®šåˆ†æµç­–ç•¥"""
        if analysis:
            # ç»“åˆåˆ†æç»“æœçš„æ™ºèƒ½åˆ†æµ
            if analysis.needs_human or analysis.urgency_level >= 5:
                return "handoff"
            elif analysis.urgency_level >= 4 and confidence < 0.6:
                return "clarification"
            elif confidence >= 0.75:
                return "direct_answer"
            elif confidence >= 0.55:
                return "clarification"
            else:
                return "handoff"
        else:
            # åŸæœ‰çš„ç®€å•åˆ†æµé€»è¾‘
            if confidence >= 0.75:
                return "direct_answer"
            elif confidence >= 0.55:
                return "clarification"
            else:
                return "handoff"
    
    def _generate_smart_response(self, msg: Message, customer, analysis, 
                               evidences: List[Evidence], confidence: float, 
                               branch: str, session) -> Tuple[str, Dict]:
        """ç”Ÿæˆæ™ºèƒ½å“åº”"""
        try:
            # æ„å»ºçŸ¥è¯†åº“ç»“æœ
            knowledge_result = {
                'documents': [ev.content for ev in evidences],
                'confidence': confidence,
                'evidence_summary': '; '.join([ev.content[:100] + '...' for ev in evidences[:3]])
            }
            
            # ç”Ÿæˆæ™ºèƒ½å›å¤
            smart_response = smart_analyzer.generate_smart_response(
                customer, msg.content, analysis, knowledge_result
            )
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§å¤„ç†
            if smart_analyzer.should_escalate(analysis, customer):
                escalation_msg = smart_analyzer.get_escalation_message(customer, analysis)
                response_text = f"{smart_response.response_text}\n\n{escalation_msg}"
                
                # æ›´æ–°å®¢æˆ·è½¬äººå·¥æ¬¡æ•°
                customer_manager.update_customer_activity(
                    customer.customer_id, handoff=True
                )
            else:
                response_text = smart_response.response_text
            
            # æ›´æ–°å®¢æˆ·æ´»åŠ¨
            customer_manager.update_customer_activity(
                customer.customer_id, 
                question_solved=(branch == "direct_answer")
            )
            
            # æ¨¡æ‹Ÿ LLM ä¿¡æ¯
            llm_info = {
                "provider": "smart_analyzer",
                "model": "enhanced_analysis",
                "token_in": len(msg.content),
                "token_out": len(response_text),
                "latency_ms": 500  # æ¨¡æ‹Ÿå»¶è¿Ÿ
            }
            
            logger.info(
                f"æ™ºèƒ½å“åº”ç”Ÿæˆå®Œæˆ: type={smart_response.response_type}, "
                f"confidence={smart_response.confidence:.2f}"
            )
            
            return response_text, llm_info
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§åˆ°åŸæœ‰é€»è¾‘
            return self._generate_response(msg, evidences, confidence, branch, session)
    
    def stop(self):
        """åœæ­¢è¿è¡Œ"""
        self.is_running = False
        self.db.close()
        logger.info("å®¢æœä¸­å°å·²åœæ­¢")


def main():
    """ä¸»å…¥å£"""
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    Path("logs").mkdir(exist_ok=True)
    Path("data").mkdir(exist_ok=True)
    
    # åˆ›å»ºå¹¶è¿è¡Œ Botï¼ˆé»˜è®¤ä½¿ç”¨ FakeAdapterï¼‰
    use_fake = os.getenv("USE_FAKE_ADAPTER", "true").lower() == "true"
    
    bot = CustomerServiceBot(use_fake=use_fake)
    bot.run()


if __name__ == "__main__":
    main()
