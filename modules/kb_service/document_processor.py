"""
æ–‡æ¡£å¤„ç†ä¸­å¿ƒ
ç»Ÿä¸€å¤„ç†å¤šç§æ ¼å¼çš„æ–‡æ¡£ï¼Œæå–æ–‡æœ¬å¹¶åˆ†æ®µ
æ”¯æŒ MCP AIOCR æœåŠ¡
"""
import logging
import re
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path

from .parsers import PDFParser, DocParser, ImageParser

logger = logging.getLogger(__name__)


class DocumentProcessor:
    """
    æ–‡æ¡£å¤„ç†ä¸­å¿ƒ
    
    åŠŸèƒ½ï¼š
    1. æ”¯æŒå¤šç§æ ¼å¼ï¼ˆPDFã€DOCã€DOCXã€å›¾ç‰‡ï¼‰
    2. è‡ªåŠ¨æå–æ–‡æœ¬
    3. æ™ºèƒ½åˆ†æ®µ
    4. ç”Ÿæˆå…ƒæ•°æ®
    """
    
    def __init__(self, use_ocr: bool = True, use_mcp_aiocr: bool = True):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            use_ocr: æ˜¯å¦å¯ç”¨æœ¬åœ°OCRï¼ˆç”¨äºæ‰«æç‰ˆPDFå’Œå›¾ç‰‡ï¼‰
            use_mcp_aiocr: æ˜¯å¦å¯ç”¨ MCP AIOCR æœåŠ¡
        """
        self.use_ocr = use_ocr
        self.use_mcp_aiocr = use_mcp_aiocr
        
        # åˆå§‹åŒ–å„ç§è§£æå™¨
        self.parsers = {
            'pdf': PDFParser(use_ocr=use_ocr),
            'doc': DocParser(),
            'image': ImageParser(lang='ch')
        }
        
        # åˆå§‹åŒ– MCP AIOCR å®¢æˆ·ç«¯
        self.mcp_aiocr = None
        if use_mcp_aiocr:
            try:
                from modules.mcp_platform import MCPManager
                mcp_manager = MCPManager()
                self.mcp_aiocr = mcp_manager.get_client("aiocr")
                logger.info("âœ… MCP AIOCR å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ MCP AIOCR åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æœ¬åœ°è§£æ: {e}")
                self.use_mcp_aiocr = False
        
        logger.info(f"æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ (OCR: {use_ocr}, MCP AIOCR: {use_mcp_aiocr})")
    
    def process_file(
        self,
        file_path: str,
        document_name: Optional[str] = None,
        document_version: str = "v1.0",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        use_mcp_aiocr: Optional[bool] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            document_name: æ–‡æ¡£åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
            document_version: æ–‡æ¡£ç‰ˆæœ¬
            chunk_size: åˆ†æ®µå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: åˆ†æ®µé‡å ï¼ˆå­—ç¬¦æ•°ï¼‰
            use_mcp_aiocr: æ˜¯å¦ä½¿ç”¨ MCP AIOCRï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€è®¾ç½®ï¼‰
        
        Returns:
            {
                'document_name': str,
                'document_version': str,
                'chunks': List[Dict],  # åˆ†æ®µåˆ—è¡¨
                'metadata': Dict,      # æ–‡æ¡£å…ƒæ•°æ®
                'processing_method': str  # å¤„ç†æ–¹æ³•ï¼š'mcp_aiocr' | 'local'
            }
        """
        path = Path(file_path)
        
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # ç¡®å®šæ–‡æ¡£åç§°
        if not document_name:
            document_name = path.stem
        
        # å†³å®šæ˜¯å¦ä½¿ç”¨ MCP AIOCR
        should_use_mcp = use_mcp_aiocr if use_mcp_aiocr is not None else self.use_mcp_aiocr
        
        # å°è¯•ä½¿ç”¨ MCP AIOCR
        if should_use_mcp and self.mcp_aiocr:
            try:
                logger.info(f"ğŸ¤– ä½¿ç”¨ MCP AIOCR å¤„ç†æ–‡æ¡£: {file_path}")
                result = asyncio.run(self._process_with_mcp_aiocr(
                    file_path, document_name, document_version, chunk_size, chunk_overlap
                ))
                if result:
                    return result
            except Exception as e:
                logger.warning(f"âš ï¸ MCP AIOCR å¤„ç†å¤±è´¥ï¼Œé™çº§åˆ°æœ¬åœ°è§£æ: {e}")
        
        # ä½¿ç”¨æœ¬åœ°è§£æå™¨
        logger.info(f"ğŸ“„ ä½¿ç”¨æœ¬åœ°è§£æå™¨å¤„ç†æ–‡æ¡£: {file_path}")
        return self._process_with_local_parser(
            file_path, document_name, document_version, chunk_size, chunk_overlap
        )
    
    async def _process_with_mcp_aiocr(
        self, file_path: str, document_name: str, document_version: str,
        chunk_size: int, chunk_overlap: int
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ MCP AIOCR å¤„ç†æ–‡æ¡£"""
        try:
            # å°è¯•æ–‡æ¡£è¯†åˆ«
            result = await self.mcp_aiocr.doc_recognition(file_path)
            
            if not result.get("success"):
                logger.warning(f"MCP AIOCR è¯†åˆ«å¤±è´¥: {result.get('error')}")
                return None
            
            text = result["content"]
            metadata = result.get("metadata", {})
            metadata.update({
                "processing_method": "mcp_aiocr",
                "aiocr_metadata": result.get("metadata", {}),
                "file_size": result.get("file_size", 0),
                "format": result.get("format", "")
            })
            
            # æ™ºèƒ½åˆ†æ®µ
            chunks = self._split_into_chunks(
                text=text,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                document_name=document_name,
                document_version=document_version
            )
            
            logger.info(f"âœ… MCP AIOCR å¤„ç†å®Œæˆ: {document_name}, {len(text)}å­—ç¬¦ â†’ {len(chunks)}ä¸ªåˆ†æ®µ")
            
            return {
                'document_name': document_name,
                'document_version': document_version,
                'chunks': chunks,
                'metadata': metadata,
                'total_chars': len(text),
                'processing_method': 'mcp_aiocr'
            }
            
        except Exception as e:
            logger.error(f"âŒ MCP AIOCR å¤„ç†å¼‚å¸¸: {e}")
            return None
    
    def _process_with_local_parser(
        self, file_path: str, document_name: str, document_version: str,
        chunk_size: int, chunk_overlap: int
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æœ¬åœ°è§£æå™¨å¤„ç†æ–‡æ¡£"""
        path = Path(file_path)
        
        # æ ¹æ®æ–‡ä»¶æ ¼å¼é€‰æ‹©è§£æå™¨
        parser = self._get_parser(path.suffix.lower())
        if not parser:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
        
        # è§£ææ–‡æ¡£
        parsed = parser.parse(file_path)
        
        text = parsed['text']
        metadata = parsed.get('metadata', {})
        metadata.update({
            "processing_method": "local"
        })
        
        # æ™ºèƒ½åˆ†æ®µ
        chunks = self._split_into_chunks(
            text=text,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            document_name=document_name,
            document_version=document_version
        )
        
        logger.info(f"âœ… æœ¬åœ°è§£æå®Œæˆ: {document_name}, {len(text)}å­—ç¬¦ â†’ {len(chunks)}ä¸ªåˆ†æ®µ")
        
        return {
            'document_name': document_name,
            'document_version': document_version,
            'chunks': chunks,
            'metadata': metadata,
            'total_chars': len(text),
            'processing_method': 'local'
        }
    
    def _get_parser(self, file_extension: str):
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–è§£æå™¨"""
        if file_extension == '.pdf':
            return self.parsers['pdf']
        elif file_extension in ['.doc', '.docx']:
            return self.parsers['doc']
        elif file_extension in ['.jpg', '.jpeg', '.png', '.bmp']:
            return self.parsers['image']
        else:
            return None
    
    def _split_into_chunks(
        self,
        text: str,
        chunk_size: int,
        chunk_overlap: int,
        document_name: str,
        document_version: str
    ) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½åˆ†æ®µ
        
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆæŒ‰ç« èŠ‚åˆ†æ®µ
        2. é•¿æ®µè½æŒ‰å­—æ•°åˆ†æ®µ
        3. ä¿ç•™ä¸Šä¸‹æ–‡é‡å 
        """
        # å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_by_paragraphs(text)
        
        chunks = []
        current_chunk = ""
        current_section = "æ­£æ–‡"
        chunk_id = 0
        
        for para in paragraphs:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            section_title = self._extract_section_title(para)
            if section_title:
                # ä¿å­˜å½“å‰chunk
                if current_chunk.strip():
                    chunks.append(self._create_chunk(
                        chunk_id=f"{document_name}_{document_version}_{chunk_id}",
                        document_name=document_name,
                        document_version=document_version,
                        section=current_section,
                        content=current_chunk.strip()
                    ))
                    chunk_id += 1
                
                current_section = section_title
                current_chunk = ""
                continue
            
            # ç´¯ç§¯æ®µè½
            if len(current_chunk) + len(para) > chunk_size:
                # å½“å‰chunkå·²æ»¡ï¼Œä¿å­˜
                if current_chunk.strip():
                    chunks.append(self._create_chunk(
                        chunk_id=f"{document_name}_{document_version}_{chunk_id}",
                        document_name=document_name,
                        document_version=document_version,
                        section=current_section,
                        content=current_chunk.strip()
                    ))
                    chunk_id += 1
                
                # å¼€å§‹æ–°chunkï¼ˆä¿ç•™é‡å ï¼‰
                if chunk_overlap > 0:
                    overlap_text = current_chunk[-chunk_overlap:]
                    current_chunk = overlap_text + para
                else:
                    current_chunk = para
            else:
                current_chunk += para + "\n"
        
        # ä¿å­˜æœ€åä¸€ä¸ªchunk
        if current_chunk.strip():
            chunks.append(self._create_chunk(
                chunk_id=f"{document_name}_{document_version}_{chunk_id}",
                document_name=document_name,
                document_version=document_version,
                section=current_section,
                content=current_chunk.strip()
            ))
        
        return chunks
    
    def _split_by_paragraphs(self, text: str) -> List[str]:
        """æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰åŒæ¢è¡Œæˆ–å¤šä¸ªæ¢è¡Œåˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _extract_section_title(self, text: str) -> Optional[str]:
        """æå–ç« èŠ‚æ ‡é¢˜"""
        # åŒ¹é…å¸¸è§çš„æ ‡é¢˜æ ¼å¼
        patterns = [
            r'^#+\s+(.+)$',  # Markdown: ## æ ‡é¢˜
            r'^\[ç¬¬.+ç« \](.+)$',  # [ç¬¬1ç« ] æ ‡é¢˜
            r'^ç¬¬.+[ç« èŠ‚]\s*[ï¼š:]\s*(.+)$',  # ç¬¬1ç« ï¼šæ ‡é¢˜
            r'^\d+\.?\s+(.+)$',  # 1. æ ‡é¢˜ æˆ– 1 æ ‡é¢˜
        ]
        
        for pattern in patterns:
            match = re.match(pattern, text.strip())
            if match:
                return match.group(1).strip()
        
        return None
    
    def _create_chunk(
        self,
        chunk_id: str,
        document_name: str,
        document_version: str,
        section: str,
        content: str
    ) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†æ®µå¯¹è±¡"""
        # æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
        keywords = self._extract_keywords(content)
        
        return {
            'chunk_id': chunk_id,
            'document_name': document_name,
            'document_version': document_version,
            'section': section,
            'content': content,
            'keywords': keywords,
            'char_count': len(content)
        }
    
    def _extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """
        æå–å…³é”®è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            text: æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
        
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤æ ‡ç‚¹
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # åˆ†è¯
        words = text.lower().split()
        
        # åœç”¨è¯
        stopwords = {
            'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬',
            'å—', 'å‘¢', 'å§', 'å•Š', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'è€Œ', 'ç­‰'
        }
        
        # è¿‡æ»¤å¹¶ç»Ÿè®¡
        word_freq = {}
        for word in words:
            if word and word not in stopwords and len(word) > 1:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æ’åºå–top_k
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords = [word for word, freq in sorted_words[:top_k]]
        
        return keywords
    
    async def batch_process_files(
        self,
        file_paths: List[str],
        document_version: str = "v1.0",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        use_mcp_aiocr: Optional[bool] = None
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            document_version: æ–‡æ¡£ç‰ˆæœ¬
            chunk_size: åˆ†æ®µå¤§å°
            chunk_overlap: åˆ†æ®µé‡å 
            use_mcp_aiocr: æ˜¯å¦ä½¿ç”¨ MCP AIOCR
        
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        results = []
        
        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        for i, file_path in enumerate(file_paths, 1):
            try:
                logger.info(f"å¤„ç†æ–‡ä»¶ {i}/{len(file_paths)}: {file_path}")
                
                result = self.process_file(
                    file_path=file_path,
                    document_version=document_version,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    use_mcp_aiocr=use_mcp_aiocr
                )
                
                results.append(result)
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}, {e}")
                results.append({
                    "error": str(e),
                    "file_path": file_path,
                    "success": False
                })
        
        success_count = sum(1 for r in results if r.get("success", True) and "error" not in r)
        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: {success_count}/{len(file_paths)} æˆåŠŸ")
        
        return results
    
    def get_supported_formats(self) -> Dict[str, List[str]]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
        local_formats = ['.pdf', '.doc', '.docx', '.jpg', '.jpeg', '.png', '.bmp']
        
        mcp_formats = []
        if self.mcp_aiocr:
            try:
                mcp_formats = self.mcp_aiocr.get_supported_formats()
            except:
                pass
        
        return {
            "local": local_formats,
            "mcp_aiocr": mcp_formats,
            "combined": list(set(local_formats + [f".{fmt}" for fmt in mcp_formats]))
        }
    
    def is_mcp_aiocr_available(self) -> bool:
        """æ£€æŸ¥ MCP AIOCR æ˜¯å¦å¯ç”¨"""
        return self.mcp_aiocr is not None and self.use_mcp_aiocr
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        health = {
            "local_parsers": {
                "pdf": "available" if self.parsers.get('pdf') else "unavailable",
                "doc": "available" if self.parsers.get('doc') else "unavailable", 
                "image": "available" if self.parsers.get('image') else "unavailable"
            },
            "mcp_aiocr": {
                "enabled": self.use_mcp_aiocr,
                "client_available": self.mcp_aiocr is not None
            }
        }
        
        # æ£€æŸ¥ MCP AIOCR å¥åº·çŠ¶æ€
        if self.mcp_aiocr:
            try:
                mcp_health = await self.mcp_aiocr.health_check()
                health["mcp_aiocr"]["status"] = mcp_health.get("status", "unknown")
            except Exception as e:
                health["mcp_aiocr"]["status"] = "error"
                health["mcp_aiocr"]["error"] = str(e)
        
        return health

