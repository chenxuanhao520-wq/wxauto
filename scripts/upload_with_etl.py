#!/usr/bin/env python3
"""
å¸¦ETLæµç¨‹çš„æ–‡æ¡£ä¸Šä¼ å·¥å…·
å®Œæ•´çš„Extract-Transform-Loadæµç¨‹
"""
import sys
import argparse
import asyncio
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def upload_document_with_etl(
    file_path: str,
    document_type: str,
    auto_fix: bool = True,
    llm_enhancement: bool = False
):
    """
    ä½¿ç”¨ETLæµç¨‹ä¸Šä¼ æ–‡æ¡£
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        document_type: æ–‡æ¡£ç±»å‹ï¼ˆproduct_info, faq, operation, technical, generalï¼‰
        auto_fix: æ˜¯å¦è‡ªåŠ¨ä¿®å¤
        llm_enhancement: æ˜¯å¦ä½¿ç”¨LLMå¢å¼º
    """
    print("=" * 80)
    print(f"ğŸ“¤ æ–‡æ¡£ä¸Šä¼  - ETLæµç¨‹å¤„ç†")
    print("=" * 80)
    print(f"æ–‡ä»¶: {file_path}")
    print(f"ç±»å‹: {document_type}")
    print(f"è‡ªåŠ¨ä¿®å¤: {'å¯ç”¨' if auto_fix else 'ç¦ç”¨'}")
    print(f"LLMå¢å¼º: {'å¯ç”¨' if llm_enhancement else 'ç¦ç”¨'}")
    print("=" * 80)
    
    try:
        # å¯¼å…¥å¿…è¦æ¨¡å—
        from modules.kb_platform.etl import DocumentETLPipeline, StructureValidator, FormatNormalizer
        from modules.kb_platform.processors.document_processor import DocumentProcessor
        from modules.kb_platform.processors.content_cleaner import ContentCleaner
        from modules.kb_platform.processors.duplicate_detector import DuplicateDetector
        from modules.kb_platform.core.quality_controller import QualityController
        
        # åˆå§‹åŒ–ç»„ä»¶
        print("\nğŸ“‹ åˆå§‹åŒ–ETLç»„ä»¶...")
        document_processor = DocumentProcessor()
        structure_validator = StructureValidator()
        format_normalizer = FormatNormalizer()
        content_cleaner = ContentCleaner()
        duplicate_detector = DuplicateDetector()
        
        # å¦‚æœå¯ç”¨LLMï¼Œåˆå§‹åŒ–AIç½‘å…³
        ai_gateway = None
        if llm_enhancement:
            try:
                from modules.ai_gateway import AIGateway
                ai_gateway = AIGateway()
                print("âœ… AIç½‘å…³å·²åˆå§‹åŒ–ï¼ˆLLMå¢å¼ºå¯ç”¨ï¼‰")
            except Exception as e:
                print(f"âš ï¸ AIç½‘å…³åˆå§‹åŒ–å¤±è´¥: {e}ï¼ŒLLMå¢å¼ºç¦ç”¨")
                llm_enhancement = False
        
        quality_controller = QualityController(
            threshold=0.75,
            strict_mode=True,
            enable_auto_fix=auto_fix,
            enable_llm_fix=llm_enhancement,
            ai_gateway=ai_gateway
        )
        
        # åˆå§‹åŒ–ETLæµæ°´çº¿
        etl_pipeline = DocumentETLPipeline(
            document_processor=document_processor,
            structure_validator=structure_validator,
            format_normalizer=format_normalizer,
            quality_controller=quality_controller,
            content_cleaner=content_cleaner,
            duplicate_detector=duplicate_detector
        )
        
        print("âœ… ETLæµæ°´çº¿åˆå§‹åŒ–å®Œæˆ")
        
        # æ‰§è¡ŒETLæµç¨‹
        print("\nğŸ”„ å¼€å§‹ETLå¤„ç†...")
        result = await etl_pipeline.process_document(
            file_path=file_path,
            document_type=document_type,
            enable_auto_fix=auto_fix,
            enable_llm_enhancement=llm_enhancement
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        if result.success:
            print("âœ… æ–‡æ¡£å¤„ç†æˆåŠŸï¼")
            print("=" * 80)
            print(f"æ–‡æ¡£ID: {result.document_id}")
            print(f"æ–‡æ¡£ç±»å‹: {result.document_type}")
            print(f"è´¨é‡åˆ†æ•°: {result.quality_score:.2f}")
            print(f"åˆ›å»ºçŸ¥è¯†å—: {len(result.transformed_chunks)}")
            print(f"å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
            
            if result.warnings:
                print(f"\nâš ï¸ è­¦å‘Š ({len(result.warnings)}):")
                for warning in result.warnings:
                    print(f"  â€¢ {warning}")
            
            print("\nğŸ“Š éªŒè¯æŠ¥å‘Š:")
            if 'structure_validation' in result.validation_report:
                sv = result.validation_report['structure_validation']
                print(f"  â€¢ ç»“æ„éªŒè¯: {'âœ… é€šè¿‡' if sv['passed'] else 'âŒ å¤±è´¥'}")
            
            if 'quality_result' in result.validation_report:
                qr = result.validation_report['quality_result']
                print(f"  â€¢ è´¨é‡æ£€æŸ¥: {'âœ… é€šè¿‡' if qr['passed'] else 'âŒ å¤±è´¥'}")
            
            if 'duplicate_result' in result.validation_report:
                dr = result.validation_report['duplicate_result']
                print(f"  â€¢ é‡å¤æ£€æµ‹: {'âœ… æ— é‡å¤' if not dr['has_duplicates'] else 'âŒ æœ‰é‡å¤'}")
            
            print("\nğŸ“¦ çŸ¥è¯†å—ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰:")
            for i, chunk in enumerate(result.transformed_chunks[:3], 1):
                print(f"\n  çŸ¥è¯†å— {i}:")
                print(f"    ID: {chunk.get('chunk_id', '')}")
                print(f"    å†…å®¹: {chunk.get('content', '')[:100]}...")
                if chunk.get('normalized'):
                    print(f"    ç»“æ„åŒ–: âœ…")
        
        else:
            print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼")
            print("=" * 80)
            print(f"æ–‡æ¡£ç±»å‹: {result.document_type}")
            print(f"è´¨é‡åˆ†æ•°: {result.quality_score:.2f}")
            
            if result.errors:
                print(f"\nâŒ é”™è¯¯ ({len(result.errors)}):")
                for error in result.errors:
                    print(f"  â€¢ {error}")
            
            if result.warnings:
                print(f"\nâš ï¸ è­¦å‘Š ({len(result.warnings)}):")
                for warning in result.warnings:
                    print(f"  â€¢ {warning}")
            
            print("\nğŸ’¡ å»ºè®®:")
            print("  1. æŸ¥çœ‹é”™è¯¯ä¿¡æ¯ï¼Œè¡¥å……ç¼ºå¤±çš„å¿…è¦å­—æ®µ")
            print("  2. å‚è€ƒæ–‡æ¡£æ¨¡æ¿: docs/kb_platform/æ–‡æ¡£æ¨¡æ¿å’Œç¤ºä¾‹.md")
            print("  3. å¯ç”¨è‡ªåŠ¨ä¿®å¤: --auto-fix")
            if not llm_enhancement:
                print("  4. å¯ç”¨LLMå¢å¼º: --llm-enhancement")
        
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¸¦ETLæµç¨‹çš„æ–‡æ¡£ä¸Šä¼ å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸Šä¼ äº§å“ä¿¡æ¯æ–‡æ¡£
  python scripts/upload_with_etl.py --file äº§å“æ‰‹å†Œ.pdf --type product_info --auto-fix
  
  # ä¸Šä¼ FAQæ–‡æ¡£ï¼ˆå¯ç”¨LLMå¢å¼ºï¼‰
  python scripts/upload_with_etl.py --file FAQ.docx --type faq --auto-fix --llm-enhancement
  
  # ä¸Šä¼ æ“ä½œæ–‡æ¡£
  python scripts/upload_with_etl.py --file å®‰è£…æŒ‡å—.pdf --type operation --auto-fix

æ–‡æ¡£ç±»å‹:
  product_info - äº§å“ä¿¡æ¯æ–‡æ¡£
  faq          - FAQå¸¸è§é—®é¢˜
  operation    - æ“ä½œæŒ‡å—æ–‡æ¡£
  technical    - æŠ€æœ¯æ–‡æ¡£
  general      - é€šç”¨æ–‡æ¡£
        """
    )
    
    parser.add_argument(
        '--file', '-f',
        required=True,
        help='æ–‡æ¡£æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--type', '-t',
        required=True,
        choices=['product_info', 'faq', 'operation', 'technical', 'general'],
        help='æ–‡æ¡£ç±»å‹'
    )
    
    parser.add_argument(
        '--auto-fix',
        action='store_true',
        help='å¯ç”¨è‡ªåŠ¨ä¿®å¤ï¼ˆè§„åˆ™+LLMï¼‰'
    )
    
    parser.add_argument(
        '--llm-enhancement',
        action='store_true',
        help='å¯ç”¨LLMå¢å¼ºï¼ˆéœ€è¦AIç½‘å…³é…ç½®ï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.file).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        sys.exit(1)
    
    # è¿è¡ŒETLæµç¨‹
    asyncio.run(upload_document_with_etl(
        file_path=args.file,
        document_type=args.type,
        auto_fix=args.auto_fix,
        llm_enhancement=args.llm_enhancement
    ))


if __name__ == "__main__":
    main()
