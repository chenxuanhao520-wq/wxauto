#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• GLM å’Œ Qwen æ¨¡å‹çš„è‡ªæˆ‘ä»‹ç»
"""
import os
import asyncio
import logging

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['QWEN_API_KEY'] = 'sk-1d7d593d85b1469683eb8e7988a0f646'
os.environ['QWEN_API_BASE'] = 'https://dashscope.aliyuncs.com/compatible-mode/v1'
os.environ['QWEN_MODEL'] = 'qwen-turbo'

os.environ['GLM_API_KEY'] = '2853e43adea74724865746c7ddfcd7ad.qp589y9s3P2KRlI4'
os.environ['GLM_API_BASE'] = 'https://open.bigmodel.cn/api/paas/v4'
os.environ['GLM_MODEL'] = 'glm-4-flash'

logging.basicConfig(
    level=logging.WARNING,  # å‡å°‘æ—¥å¿—è¾“å‡º
    format='%(message)s'
)

logger = logging.getLogger(__name__)


async def test_glm_identity():
    """æµ‹è¯• GLM çš„è‡ªæˆ‘è®¤çŸ¥"""
    print("\n" + "="*70)
    print("æµ‹è¯• 1: GLM (æ™ºè°±AI) - ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ")
    print("="*70)
    
    from modules.ai_gateway.providers.glm_provider import GLMProvider
    from modules.ai_gateway.types import ProviderConfig, LLMRequest
    
    config = ProviderConfig(
        name="glm",
        api_key=os.getenv('GLM_API_KEY'),
        api_base=os.getenv('GLM_API_BASE'),
        model=os.getenv('GLM_MODEL'),
        timeout=30
    )
    
    provider = GLMProvider(config)
    
    request = LLMRequest(
        user_message="ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿè¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        max_tokens=300,
        temperature=0.7
    )
    
    response = await asyncio.to_thread(provider.generate, request)
    
    print(f"\nğŸ“ æ¨¡å‹: {response.model}")
    print(f"â±ï¸  å»¶è¿Ÿ: {response.latency_ms}ms")
    print(f"ğŸ« Token: {response.token_in} (è¾“å…¥) / {response.token_out} (è¾“å‡º) / {response.token_total} (æ€»è®¡)")
    print(f"\nğŸ’¬ GLM å›å¤:\n")
    print("-" * 70)
    print(response.content)
    print("-" * 70)


async def test_qwen_identity():
    """æµ‹è¯• Qwen çš„è‡ªæˆ‘è®¤çŸ¥"""
    print("\n" + "="*70)
    print("æµ‹è¯• 2: Qwen (é€šä¹‰åƒé—®) - ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ")
    print("="*70)
    
    from modules.ai_gateway.providers.qwen_provider import QwenProvider
    from modules.ai_gateway.types import ProviderConfig, LLMRequest
    
    config = ProviderConfig(
        name="qwen",
        api_key=os.getenv('QWEN_API_KEY'),
        api_base=os.getenv('QWEN_API_BASE'),
        model=os.getenv('QWEN_MODEL'),
        timeout=30
    )
    
    provider = QwenProvider(config)
    
    request = LLMRequest(
        user_message="ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿè¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        max_tokens=300,
        temperature=0.7
    )
    
    response = await asyncio.to_thread(provider.generate, request)
    
    print(f"\nğŸ“ æ¨¡å‹: {response.model}")
    print(f"â±ï¸  å»¶è¿Ÿ: {response.latency_ms}ms")
    print(f"ğŸ« Token: {response.token_in} (è¾“å…¥) / {response.token_out} (è¾“å‡º) / {response.token_total} (æ€»è®¡)")
    print(f"\nğŸ’¬ Qwen å›å¤:\n")
    print("-" * 70)
    print(response.content)
    print("-" * 70)


async def compare_models():
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
    print("\n" + "="*70)
    print("æµ‹è¯• 3: æ¨¡å‹å¯¹æ¯”")
    print("="*70)
    
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print("-" * 70)
    print(f"{'æŒ‡æ ‡':<15} {'GLM-4-flash':<25} {'Qwen-turbo':<25}")
    print("-" * 70)
    print(f"{'æä¾›å•†':<15} {'æ™ºè°±AI':<25} {'é˜¿é‡Œäº‘':<25}")
    print(f"{'å®šä½':<15} {'å…è´¹å¿«é€Ÿæ¨¡å‹':<25} {'å…è´¹é€šç”¨æ¨¡å‹':<25}")
    print(f"{'ä»·æ ¼':<15} {'å…è´¹':<25} {'æœ‰å…è´¹é¢åº¦':<25}")
    print(f"{'é€Ÿåº¦':<15} {'âš¡âš¡âš¡ æå¿«':<25} {'âš¡âš¡ å¿«':<25}")
    print(f"{'é€‚ç”¨åœºæ™¯':<15} {'ç®€å•å¯¹è¯ã€å¿«é€Ÿé—®ç­”':<25} {'é€šç”¨å¯¹è¯ã€æŠ€æœ¯æ”¯æŒ':<25}")


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸ¤–"*35)
    print("æ¨¡å‹èº«ä»½æµ‹è¯• - ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ")
    print("ğŸ¤–"*35)
    
    # æµ‹è¯• GLM
    await test_glm_identity()
    
    # ç­‰å¾…ä¸€ä¸‹
    await asyncio.sleep(1)
    
    # æµ‹è¯• Qwen
    await test_qwen_identity()
    
    # å¯¹æ¯”
    await compare_models()
    
    print("\n" + "="*70)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("="*70)
    print("\nä¸¤ä¸ªæ¨¡å‹éƒ½å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
    print("ç°åœ¨æ‚¨å¯ä»¥æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š")
    print("  â€¢ GLM-4-flash: é€Ÿåº¦å¿«ã€å…è´¹ã€é€‚åˆç®€å•å¯¹è¯")
    print("  â€¢ Qwen-turbo: é€šç”¨æ€§å¼ºã€å…è´¹é¢åº¦ã€é€‚åˆæŠ€æœ¯æ”¯æŒ")
    print("")


if __name__ == "__main__":
    asyncio.run(main())

