#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®åœºæ™¯æµ‹è¯• - å®Œæ•´æµç¨‹
æµ‹è¯• GLM å’Œ Qwen åœ¨å®¢æœåœºæ™¯ä¸‹çš„è¡¨ç°
"""
import os
import asyncio
import logging
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['QWEN_API_KEY'] = 'sk-1d7d593d85b1469683eb8e7988a0f646'
os.environ['GLM_API_KEY'] = '2853e43adea74724865746c7ddfcd7ad.qp589y9s3P2KRlI4'
os.environ['JWT_SECRET_KEY'] = 'test-secret-key'
os.environ['VALID_AGENT_CREDENTIALS'] = 'agent_001:test-key'

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

# æµ‹è¯•é—®é¢˜é›†ï¼ˆè¦†ç›–ä¸åŒéš¾åº¦å’Œåœºæ™¯ï¼‰
TEST_CASES = [
    {
        "category": "ç®€å•é—®ç­”",
        "question": "ä½ å¥½ï¼Œåœ¨å—ï¼Ÿ",
        "kb_context": None,
        "expected_behavior": "å¿«é€Ÿå“åº”ï¼Œç®€æ´å›å¤"
    },
    {
        "category": "äº§å“å’¨è¯¢",
        "question": "ä½ ä»¬çš„å……ç”µæ¡©æ”¯æŒå¤šå°‘åŠŸç‡ï¼Ÿ",
        "kb_context": """
        ã€äº§å“æ‰‹å†Œ v2.0ã€‘å……ç”µæ¡©æŠ€æœ¯å‚æ•°
        - å‹å·: CP-7KW-AC
        - åŠŸç‡: 7KW
        - è¾“å…¥ç”µå‹: 220V
        - è¾“å‡ºç”µæµ: 32A
        - å……ç”µæ¥å£: å›½æ ‡7å­”
        """,
        "expected_behavior": "å‡†ç¡®å¼•ç”¨çŸ¥è¯†åº“ï¼Œæä¾›æŠ€æœ¯å‚æ•°"
    },
    {
        "category": "æ•…éšœæ’æŸ¥",
        "question": "å……ç”µæ¡©å±å¹•ä¸äº®äº†ï¼Œæ€ä¹ˆåŠï¼Ÿ",
        "kb_context": """
        ã€æ•…éšœæ’æŸ¥æ‰‹å†Œ v1.5ã€‘å¸¸è§é—®é¢˜
        Q: å±å¹•ä¸äº®
        A: æ’æŸ¥æ­¥éª¤ï¼š
        â‘ æ£€æŸ¥ç”µæºå¼€å…³æ˜¯å¦æ‰“å¼€
        â‘¡æ£€æŸ¥æ–­è·¯å™¨æ˜¯å¦è·³é—¸
        â‘¢é‡å¯è®¾å¤‡ï¼ˆå…³é—­30ç§’åå†æ‰“å¼€ï¼‰
        â‘£å¦‚ä»æ— æ³•è§£å†³ï¼Œè”ç³»å”®åï¼š400-123-4567
        """,
        "expected_behavior": "åˆ†æ­¥éª¤è¯´æ˜ï¼Œå¼•ç”¨æ–‡æ¡£"
    },
    {
        "category": "å¤æ‚æ¨ç†",
        "question": "æˆ‘æœ‰3å°7KWå……ç”µæ¡©ï¼Œæ¯å¤©å……ç”µ8å°æ—¶ï¼Œä¸€ä¸ªæœˆç”µè´¹å¤§æ¦‚å¤šå°‘é’±ï¼Ÿï¼ˆæŒ‰0.6å…ƒ/åº¦è®¡ç®—ï¼‰",
        "kb_context": "ã€äº§å“æ‰‹å†Œã€‘7KWå……ç”µæ¡©åŠŸç‡ä¸º7åƒç“¦",
        "expected_behavior": "æ•°å­¦è®¡ç®— + æ¨ç†èƒ½åŠ›"
    },
    {
        "category": "å”®åæ”¿ç­–",
        "question": "ä¿ä¿®æœŸæ˜¯å¤šä¹…ï¼Ÿ",
        "kb_context": """
        ã€å”®åæ”¿ç­– v3.0ã€‘
        - æ•´æœºä¿ä¿®ï¼š2å¹´
        - æ ¸å¿ƒéƒ¨ä»¶ä¿ä¿®ï¼š3å¹´
        - äººä¸ºæŸåä¸åœ¨ä¿ä¿®èŒƒå›´
        - å…è´¹ä¸Šé—¨ï¼šä¿ä¿®æœŸå†…
        """,
        "expected_behavior": "å‡†ç¡®æå–å…³é”®ä¿¡æ¯"
    },
    {
        "category": "å¤šè½®å¯¹è¯",
        "question": "å……ç”µæ¡©æ€ä¹ˆå®‰è£…ï¼Ÿ",
        "kb_context": """
        ã€å®‰è£…æŒ‡å— v2.1ã€‘
        1. é€‰å€ï¼šé è¿‘é…ç”µç®±ï¼Œåœ°é¢å¹³æ•´
        2. å¸ƒçº¿ï¼šé¢„åŸ‹ç”µç¼†ç®¡ï¼Œé“ºè®¾ç”µç¼†
        3. å®‰è£…ï¼šå›ºå®šåº•åº§ï¼Œè¿æ¥ç”µæº
        4. è°ƒè¯•ï¼šé€šç”µæµ‹è¯•ï¼ŒéªŒæ”¶
        æ³¨æ„ï¼šéœ€ä¸“ä¸šç”µå·¥æ“ä½œ
        """,
        "expected_behavior": "ç»“æ„åŒ–å›ç­”ï¼Œå¼•ç”¨æ–‡æ¡£"
    }
]


async def test_model_with_case(provider_name: str, model_name: str, test_case: dict):
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„å•ä¸ªåœºæ™¯"""
    from modules.ai_gateway.gateway import AIGateway
    
    # åˆ›å»ºç½‘å…³ï¼ˆä½¿ç”¨æŒ‡å®šæ¨¡å‹ï¼‰
    gateway = AIGateway(
        primary_provider=provider_name,
        primary_model=model_name,
        enable_fallback=False,
        enable_smart_routing=False
    )
    
    # è°ƒç”¨
    response = await gateway.generate(
        user_message=test_case['question'],
        evidence_context=test_case['kb_context'],
        max_tokens=400,
        temperature=0.7
    )
    
    return response


async def run_comprehensive_test():
    """è¿è¡Œå…¨é¢æµ‹è¯•"""
    print("\n" + "="*80)
    print("ğŸ§ª çœŸå®åœºæ™¯æµ‹è¯• - å®¢æœç³»ç»Ÿå®Œæ•´æµç¨‹")
    print("="*80)
    print("\næµ‹è¯•åœºæ™¯: 6ä¸ªçœŸå®å®¢æœé—®é¢˜")
    print("æµ‹è¯•æ¨¡å‹: GLM-4-Flash vs Qwen-Turbo")
    print("")
    
    results = {
        'glm': [],
        'qwen': []
    }
    
    for i, test_case in enumerate(TEST_CASES, 1):
        print("\n" + "â”"*80)
        print(f"ğŸ“‹ æµ‹è¯• {i}/{len(TEST_CASES)}: {test_case['category']}")
        print("â”"*80)
        print(f"\nâ“ ç”¨æˆ·é—®é¢˜: {test_case['question']}")
        
        if test_case['kb_context']:
            print(f"\nğŸ“š çŸ¥è¯†åº“ä¸Šä¸‹æ–‡: {test_case['kb_context'][:80]}...")
        
        print(f"\nğŸ¯ æœŸæœ›è¡Œä¸º: {test_case['expected_behavior']}")
        print("")
        
        # æµ‹è¯• GLM
        print("â”Œâ”€ GLM-4-Flash â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        try:
            glm_response = await test_model_with_case('glm', 'glm-4-flash', test_case)
            print(f"â”‚ â±ï¸  å»¶è¿Ÿ: {glm_response.latency_ms}ms")
            print(f"â”‚ ğŸ« Token: {glm_response.token_total}")
            print(f"â”‚")
            print(f"â”‚ ğŸ’¬ å›ç­”:")
            for line in glm_response.content.split('\n'):
                print(f"â”‚ {line}")
            print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            results['glm'].append({
                'case': test_case['category'],
                'latency': glm_response.latency_ms,
                'tokens': glm_response.token_total,
                'content': glm_response.content
            })
        except Exception as e:
            print(f"â”‚ âŒ å¤±è´¥: {e}")
            print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        print("")
        
        # æµ‹è¯• Qwen
        print("â”Œâ”€ Qwen-Turbo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        try:
            qwen_response = await test_model_with_case('qwen', 'qwen-turbo', test_case)
            print(f"â”‚ â±ï¸  å»¶è¿Ÿ: {qwen_response.latency_ms}ms")
            print(f"â”‚ ğŸ« Token: {qwen_response.token_total}")
            print(f"â”‚")
            print(f"â”‚ ğŸ’¬ å›ç­”:")
            for line in qwen_response.content.split('\n'):
                print(f"â”‚ {line}")
            print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            results['qwen'].append({
                'case': test_case['category'],
                'latency': qwen_response.latency_ms,
                'tokens': qwen_response.token_total,
                'content': qwen_response.content
            })
        except Exception as e:
            print(f"â”‚ âŒ å¤±è´¥: {e}")
            print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(1)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\n\n" + "="*80)
    print("ğŸ“Š ç»¼åˆå¯¹æ¯”æŠ¥å‘Š")
    print("="*80)
    
    # è®¡ç®—å¹³å‡å€¼
    glm_avg_latency = sum(r['latency'] for r in results['glm']) / len(results['glm'])
    glm_avg_tokens = sum(r['tokens'] for r in results['glm']) / len(results['glm'])
    
    qwen_avg_latency = sum(r['latency'] for r in results['qwen']) / len(results['qwen'])
    qwen_avg_tokens = sum(r['tokens'] for r in results['qwen']) / len(results['qwen'])
    
    print(f"\n{'æŒ‡æ ‡':<20} {'GLM-4-Flash':<25} {'Qwen-Turbo':<25}")
    print("-"*80)
    print(f"{'å¹³å‡å»¶è¿Ÿ':<20} {glm_avg_latency:<25.0f}ms {qwen_avg_latency:<25.0f}ms")
    print(f"{'å¹³å‡ Token æ•°':<20} {glm_avg_tokens:<25.0f} {qwen_avg_tokens:<25.0f}")
    print(f"{'ä»·æ ¼':<20} {'å…è´¹':<25} {'æœ‰å…è´¹é¢åº¦':<25}")
    print(f"{'ç¨³å®šæ€§':<20} {'âœ… 6/6':<25} {'âœ… 6/6':<25}")
    
    # é€Ÿåº¦å¯¹æ¯”
    print(f"\né€Ÿåº¦å¯¹æ¯”:")
    if glm_avg_latency < qwen_avg_latency:
        diff = ((qwen_avg_latency - glm_avg_latency) / qwen_avg_latency) * 100
        print(f"  ğŸ† GLM æ›´å¿« ({diff:.1f}%)")
    else:
        diff = ((glm_avg_latency - qwen_avg_latency) / glm_avg_latency) * 100
        print(f"  ğŸ† Qwen æ›´å¿« ({diff:.1f}%)")
    
    # Token æ•ˆç‡
    print(f"\nToken æ•ˆç‡:")
    if glm_avg_tokens < qwen_avg_tokens:
        print(f"  ğŸ† GLM æ›´ç²¾ç®€ (å¹³å‡èŠ‚çœ {qwen_avg_tokens - glm_avg_tokens:.0f} tokens)")
    else:
        print(f"  ğŸ† Qwen æ›´è¯¦ç»† (å¹³å‡å¤š {qwen_avg_tokens - glm_avg_tokens:.0f} tokens)")
    
    # åœºæ™¯å»ºè®®
    print("\n\n" + "="*80)
    print("ğŸ’¡ ä½¿ç”¨å»ºè®®")
    print("="*80)
    
    print("""
åœºæ™¯ 1: ç®€å•é—®ç­”ï¼ˆä½ å¥½ã€åœ¨å—ç­‰ï¼‰
  æ¨è: GLM-4-Flash
  åŸå› : å®Œå…¨å…è´¹ï¼Œé€Ÿåº¦å¿«

åœºæ™¯ 2: äº§å“å’¨è¯¢ï¼ˆéœ€è¦å¼•ç”¨çŸ¥è¯†åº“ï¼‰
  æ¨è: Qwen-Turbo
  åŸå› : å›ç­”æ›´è¯¦ç»†ï¼Œå¼•ç”¨æ›´å‡†ç¡®

åœºæ™¯ 3: æ•…éšœæ’æŸ¥ï¼ˆéœ€è¦åˆ†æ­¥éª¤è¯´æ˜ï¼‰
  æ¨è: Qwen-Turbo
  åŸå› : ç»“æ„åŒ–èƒ½åŠ›æ›´å¼º

åœºæ™¯ 4: å¤æ‚æ¨ç†ï¼ˆè®¡ç®—ã€é€»è¾‘ï¼‰
  æ¨è: Qwen-Turbo
  åŸå› : æ¨ç†èƒ½åŠ›æ›´å¼º

åœºæ™¯ 5: æ”¿ç­–æŸ¥è¯¢ï¼ˆéœ€è¦å‡†ç¡®æå–ä¿¡æ¯ï¼‰
  æ¨è: ä¸¤è€…å‡å¯
  åŸå› : éƒ½èƒ½å‡†ç¡®æå–å…³é”®ä¿¡æ¯

åœºæ™¯ 6: å¤šè½®å¯¹è¯
  æ¨è: Qwen-Turbo
  åŸå› : ä¸Šä¸‹æ–‡ç†è§£æ›´å¥½
    """)
    
    print("="*80)
    print("ğŸ¯ æ™ºèƒ½è·¯ç”±ç­–ç•¥å»ºè®®")
    print("="*80)
    print("""
å¤æ‚åº¦ < 0.3 (ç®€å•é—®ç­”):
  â†’ GLM-4-Flash (å…è´¹+å¿«é€Ÿ)

å¤æ‚åº¦ 0.3-0.6 (ä¸€èˆ¬å’¨è¯¢):
  â†’ Qwen-Turbo (å¹³è¡¡æ€§èƒ½)

å¤æ‚åº¦ > 0.6 (å¤æ‚æ¨ç†):
  â†’ Qwen-Turbo (æ¨ç†èƒ½åŠ›å¼º)

å¤±è´¥é™çº§:
  ä¸»æ¨¡å‹å¤±è´¥ â†’ è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨æ¨¡å‹
    """)


if __name__ == "__main__":
    print("\nğŸš€ å¼€å§‹çœŸå®åœºæ™¯æµ‹è¯•...")
    print("è¿™å°†æµ‹è¯•ç³»ç»Ÿåœ¨å®é™…å®¢æœåœºæ™¯ä¸‹çš„è¡¨ç°")
    print("é¢„è®¡è€—æ—¶: çº¦ 1-2 åˆ†é’Ÿ\n")
    
    asyncio.run(run_comprehensive_test())
    
    print("\n" + "="*80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*80)
    print("\nä¸¤ä¸ªæ¨¡å‹éƒ½è¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥æ ¹æ®åœºæ™¯é€‰æ‹©ï¼")
    print("å»ºè®®å¯ç”¨æ™ºèƒ½è·¯ç”±ï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹ã€‚")
    print("")

