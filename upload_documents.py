#!/usr/bin/env python3
"""
çŸ¥è¯†åº“æ–‡æ¡£ä¸Šä¼ å·¥å…·
æ”¯æŒæ‰¹é‡ä¸Šä¼  PDFã€DOCã€DOCXã€å›¾ç‰‡ç­‰æ ¼å¼
"""
import sys
import argparse
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from kb_service.document_processor import DocumentProcessor
from kb_service.embeddings import BGEM3Embeddings
from kb_service.vector_store import ChromaStore


def print_header(text):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"  {text}")
    print("=" * 60 + "\n")


def upload_single_file(
    file_path: str,
    document_name: str = None,
    version: str = "v1.0",
    use_vector: bool = True
):
    """
    ä¸Šä¼ å•ä¸ªæ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        document_name: æ–‡æ¡£åç§°
        version: ç‰ˆæœ¬
        use_vector: æ˜¯å¦ä½¿ç”¨å‘é‡æ£€ç´¢
    """
    print_header(f"ä¸Šä¼ æ–‡æ¡£: {file_path}")
    
    try:
        # 1. å¤„ç†æ–‡æ¡£
        print("æ­¥éª¤1ï¼šè§£ææ–‡æ¡£...")
        processor = DocumentProcessor(use_ocr=True)
        result = processor.process_file(
            file_path=file_path,
            document_name=document_name,
            document_version=version,
            chunk_size=500,
            chunk_overlap=50
        )
        
        print(f"  âœ… æ–‡æ¡£åç§°: {result['document_name']}")
        print(f"  âœ… ç‰ˆæœ¬: {result['document_version']}")
        print(f"  âœ… æ€»å­—ç¬¦: {result['total_chars']}")
        print(f"  âœ… åˆ†æ®µæ•°: {len(result['chunks'])}")
        
        # 2. ä¿å­˜åˆ°SQLiteï¼ˆç®€å•æ¨¡å¼ï¼‰
        if not use_vector:
            print("\næ­¥éª¤2ï¼šä¿å­˜åˆ°SQLite...")
            from storage.db import Database
            import sqlite3
            
            db = Database("data/data.db")
            conn = db.connect()
            cursor = conn.cursor()
            
            for chunk in result['chunks']:
                keywords_str = ','.join(chunk.get('keywords', []))
                
                cursor.execute("""
                    INSERT OR REPLACE INTO knowledge_chunks
                    (chunk_id, document_name, document_version, section, content, keywords)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    chunk['chunk_id'],
                    chunk['document_name'],
                    chunk['document_version'],
                    chunk['section'],
                    chunk['content'],
                    keywords_str
                ))
            
            conn.commit()
            db.close()
            
            print(f"  âœ… å·²ä¿å­˜åˆ° SQLite: {len(result['chunks'])} ä¸ªåˆ†æ®µ")
        
        # 3. ç”Ÿæˆå‘é‡å¹¶ä¿å­˜ï¼ˆå‘é‡æ¨¡å¼ï¼‰
        else:
            print("\næ­¥éª¤2ï¼šç”Ÿæˆå‘é‡åµŒå…¥...")
            
            try:
                embeddings_model = BGEM3Embeddings()
                
                # æå–æ–‡æœ¬
                texts = [chunk['content'] for chunk in result['chunks']]
                metadatas = [
                    {
                        'document_name': chunk['document_name'],
                        'document_version': chunk['document_version'],
                        'section': chunk['section'],
                        'keywords': ','.join(chunk.get('keywords', []))
                    }
                    for chunk in result['chunks']
                ]
                ids = [chunk['chunk_id'] for chunk in result['chunks']]
                
                # ç”ŸæˆåµŒå…¥
                print(f"  ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆä½¿ç”¨BGE-M3ï¼‰...")
                embeddings = embeddings_model.embed_documents(texts)
                
                print(f"  âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ: {len(embeddings)} ä¸ªå‘é‡")
                
                # ä¿å­˜åˆ°å‘é‡æ•°æ®åº“
                print("\næ­¥éª¤3ï¼šä¿å­˜åˆ°å‘é‡æ•°æ®åº“...")
                vector_store = ChromaStore(persist_directory="data/chroma_db")
                
                vector_store.add_documents(
                    collection_name="knowledge_base",
                    texts=texts,
                    metadatas=metadatas,
                    ids=ids,
                    embeddings=embeddings
                )
                
                count = vector_store.get_count("knowledge_base")
                print(f"  âœ… å·²ä¿å­˜åˆ° Chroma: å½“å‰æ€»è®¡ {count} ä¸ªåˆ†æ®µ")
                
            except ImportError as e:
                print(f"  âš ï¸  å‘é‡åº“æœªå®‰è£…: {e}")
                print("  å›é€€åˆ°ç®€å•æ¨¡å¼ï¼ˆä»…ä¿å­˜åˆ°SQLiteï¼‰...")
                upload_single_file(file_path, document_name, version, use_vector=False)
                return
        
        print("\n" + "=" * 60)
        print("âœ… æ–‡æ¡£ä¸Šä¼ å®Œæˆï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ ä¸Šä¼ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def upload_directory(
    directory: str,
    version: str = "v1.0",
    pattern: str = "*",
    use_vector: bool = True
):
    """
    æ‰¹é‡ä¸Šä¼ ç›®å½•ä¸­çš„æ–‡ä»¶
    
    Args:
        directory: ç›®å½•è·¯å¾„
        version: ç»Ÿä¸€ç‰ˆæœ¬å·
        pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        use_vector: æ˜¯å¦ä½¿ç”¨å‘é‡æ£€ç´¢
    """
    print_header(f"æ‰¹é‡ä¸Šä¼ ç›®å½•: {directory}")
    
    path = Path(directory)
    if not path.is_dir():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory}")
        return
    
    # æ”¯æŒçš„æ ¼å¼
    supported_extensions = ['.pdf', '.doc', '.docx', '.jpg', '.jpeg', '.png']
    
    # æŸ¥æ‰¾æ–‡ä»¶
    files = []
    for ext in supported_extensions:
        files.extend(path.glob(f"**/*{ext}"))
    
    if not files:
        print(f"âŒ æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶")
        print(f"   æ”¯æŒæ ¼å¼: {', '.join(supported_extensions)}")
        return
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶:\n")
    for f in files:
        print(f"  - {f.name}")
    
    print("\nå¼€å§‹ä¸Šä¼ ...\n")
    
    # é€ä¸ªä¸Šä¼ 
    success_count = 0
    for file in files:
        try:
            upload_single_file(
                file_path=str(file),
                document_name=file.stem,
                version=version,
                use_vector=use_vector
            )
            success_count += 1
        except Exception as e:
            print(f"âŒ {file.name} ä¸Šä¼ å¤±è´¥: {e}")
            continue
    
    print("\n" + "=" * 60)
    print(f"æ‰¹é‡ä¸Šä¼ å®Œæˆ: {success_count}/{len(files)} ä¸ªæ–‡ä»¶æˆåŠŸ")
    print("=" * 60)


def list_documents(use_vector: bool = True):
    """åˆ—å‡ºå·²ä¸Šä¼ çš„æ–‡æ¡£"""
    print_header("çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨")
    
    if use_vector:
        try:
            vector_store = ChromaStore(persist_directory="data/chroma_db")
            count = vector_store.get_count("knowledge_base")
            
            print(f"å‘é‡æ•°æ®åº“ä¸­å…±æœ‰ {count} ä¸ªçŸ¥è¯†åˆ†æ®µ")
            print("ï¼ˆChromaä¸æ”¯æŒç›´æ¥åˆ—å‡ºæ–‡æ¡£ï¼Œè¯·ä½¿ç”¨SQLiteæ¨¡å¼æŸ¥çœ‹è¯¦æƒ…ï¼‰")
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
    
    # ä»SQLiteæŸ¥è¯¢
    try:
        import sqlite3
        
        conn = sqlite3.connect("data/data.db")
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT DISTINCT document_name, document_version, COUNT(*) as chunk_count
            FROM knowledge_chunks
            GROUP BY document_name, document_version
            ORDER BY document_name
        """)
        
        rows = cursor.fetchall()
        conn.close()
        
        if rows:
            print(f"\nSQLiteæ•°æ®åº“ä¸­çš„æ–‡æ¡£:\n")
            for name, version, count in rows:
                print(f"  ğŸ“„ {name} {version} ({count} ä¸ªåˆ†æ®µ)")
        else:
            print("\nSQLiteæ•°æ®åº“ä¸­æš‚æ— æ–‡æ¡£")
            
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢SQLiteå¤±è´¥: {e}")


def search_test(query: str, use_vector: bool = True):
    """æµ‹è¯•æ£€ç´¢"""
    print_header(f"æµ‹è¯•æ£€ç´¢: {query}")
    
    if use_vector:
        try:
            # ä½¿ç”¨å‘é‡æ£€ç´¢
            from kb_service.embeddings import BGEM3Embeddings
            from kb_service.vector_store import ChromaStore
            
            print("ä½¿ç”¨å‘é‡æ£€ç´¢ï¼ˆBGE-M3ï¼‰...\n")
            
            embeddings_model = BGEM3Embeddings()
            query_embedding = embeddings_model.embed_query(query)
            
            vector_store = ChromaStore(persist_directory="data/chroma_db")
            results = vector_store.search(
                collection_name="knowledge_base",
                query_embedding=query_embedding,
                n_results=5
            )
            
            if results and results['documents']:
                documents = results['documents'][0]
                metadatas = results['metadatas'][0]
                distances = results['distances'][0]
                
                print(f"æ‰¾åˆ° {len(documents)} æ¡ç›¸å…³ç»“æœ:\n")
                
                for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances), 1):
                    score = 1 - dist  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                    print(f"{i}. ã€{meta['document_name']} {meta.get('document_version', '')} - {meta.get('section', '')}ã€‘")
                    print(f"   ç›¸ä¼¼åº¦: {score:.3f}")
                    print(f"   å†…å®¹: {doc[:100]}...")
                    print()
            else:
                print("æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                
        except ImportError as e:
            print(f"âš ï¸  å‘é‡æ£€ç´¢ä¸å¯ç”¨: {e}")
            print("å›é€€åˆ°BM25æ£€ç´¢...\n")
            use_vector = False
    
    if not use_vector:
        # ä½¿ç”¨BM25æ£€ç´¢
        from rag.retriever import Retriever
        
        print("ä½¿ç”¨BM25æ£€ç´¢...\n")
        
        retriever = Retriever()
        retriever.load_knowledge_base("data/data.db")
        
        evidences = retriever.retrieve(query, k=5)
        confidence = retriever.calculate_confidence(evidences)
        
        print(f"ç½®ä¿¡åº¦: {confidence:.2f}\n")
        print(f"æ‰¾åˆ° {len(evidences)} æ¡ç›¸å…³ç»“æœ:\n")
        
        for i, ev in enumerate(evidences, 1):
            print(f"{i}. ã€{ev.document_name} {ev.document_version} - {ev.section}ã€‘")
            print(f"   å¾—åˆ†: {ev.score:.2f}")
            print(f"   å†…å®¹: {ev.content[:100]}...")
            print()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çŸ¥è¯†åº“æ–‡æ¡£ä¸Šä¼ å·¥å…·')
    parser.add_argument('action', choices=['upload', 'upload-dir', 'list', 'search'],
                       help='æ“ä½œç±»å‹')
    parser.add_argument('--file', help='æ–‡ä»¶è·¯å¾„ï¼ˆuploadï¼‰')
    parser.add_argument('--dir', help='ç›®å½•è·¯å¾„ï¼ˆupload-dirï¼‰')
    parser.add_argument('--name', help='æ–‡æ¡£åç§°ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--version', default='v1.0', help='ç‰ˆæœ¬å·')
    parser.add_argument('--query', help='æœç´¢æŸ¥è¯¢ï¼ˆsearchï¼‰')
    parser.add_argument('--mode', choices=['vector', 'simple'], default='vector',
                       help='æ¨¡å¼ï¼švector=å‘é‡æ£€ç´¢, simple=BM25æ£€ç´¢')
    
    args = parser.parse_args()
    
    use_vector = (args.mode == 'vector')
    
    print("\n" + "ğŸ“š " * 20)
    print("  çŸ¥è¯†åº“æ–‡æ¡£ä¸Šä¼ å·¥å…·")
    print("ğŸ“š " * 20)
    
    if args.action == 'upload':
        if not args.file:
            print("âŒ è¯·æŒ‡å®š --file å‚æ•°")
            sys.exit(1)
        
        upload_single_file(
            file_path=args.file,
            document_name=args.name,
            version=args.version,
            use_vector=use_vector
        )
    
    elif args.action == 'upload-dir':
        if not args.dir:
            print("âŒ è¯·æŒ‡å®š --dir å‚æ•°")
            sys.exit(1)
        
        upload_directory(
            directory=args.dir,
            version=args.version,
            use_vector=use_vector
        )
    
    elif args.action == 'list':
        list_documents(use_vector=use_vector)
    
    elif args.action == 'search':
        if not args.query:
            print("âŒ è¯·æŒ‡å®š --query å‚æ•°")
            sys.exit(1)
        
        search_test(query=args.query, use_vector=use_vector)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  å·²ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

